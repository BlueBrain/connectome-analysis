{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"connalysis: Functions to analyze connectomes","text":"<p>This package provides a library of general functions to analyze connectomes. Functions are divided into two groups:</p> <ul> <li>Modelling: Functions to construct connectomes with specific properties</li> <li>Network: Graph theoretic and topological functions evaluated on connectomes</li> <li>Randomization: Graph theoretic and topological functions evaluated on connectomes</li> </ul>"},{"location":"modelling/","title":"Functions for modelling","text":"<p>This page describes functions contained in the <code>modelling.py</code> file.</p>"},{"location":"modelling/#src.connalysis.modelling.modelling.aa_test_func","title":"<code>aa_test_func(adj_matrix, size, **kwargs)</code>","text":"<p>General probability model building, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj_matrix</code> <code>array_like</code> <p>Adjacency matrix of the circuit</p> required <code>size</code> <code>int</code> <p>number of neurons</p> required <code>**kwargs</code> <code>dict, optional</code> <p>Extra arguments to pass to function</p> <code>{}</code> <p>Returns:</p> Type Description <code>array_like</code> <p>A new adjacency matrix</p> <p>Examples:</p> <p>A comment explaining this example.</p> <p>adj = np.array([0,1,1],[0,0,1],[0,0,0]) test_func(adj, 3) np.array([[0,0,0],[1,0,0],[1,1,0]])</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the dataframe is missing information about neurons</p>"},{"location":"modelling/#src.connalysis.modelling.modelling.aa_test_func--notes","title":"Notes","text":"<p>Math comments are not working yet.  This is working now. Still references are not linking to each other from notes to references. [1]_</p> \\[ X(e^{j\\omega } ) = x(n)e^{ - j\\omega n} \\]"},{"location":"modelling/#src.connalysis.modelling.modelling.aa_test_func--see-also","title":"See Also","text":"<p>test_newfunc : A variant of this function</p>"},{"location":"modelling/#src.connalysis.modelling.modelling.aa_test_func--references","title":"References","text":"<p>.. [1] Author A, Author B, \"Title,\" Journal, volume, pages, year.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def aa_test_func(adj_matrix, size, **kwargs):\n\"\"\"General probability model building, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj_matrix : array_like\n        Adjacency matrix of the circuit\n    size : int\n        number of neurons\n    **kwargs : dict, optional\n        Extra arguments to pass to function\n\n    Returns\n    -------\n    array_like\n        A new adjacency matrix\n\n    Examples\n    --------\n    A comment explaining this example.\n    &gt; adj = np.array([0,1,1],[0,0,1],[0,0,0])\n    &gt; test_func(adj, 3)\n    np.array([[0,0,0],[1,0,0],[1,1,0]])\n\n    Raises\n    ------\n    KeyError\n        If the dataframe is missing information about neurons\n\n    Notes\n    -----\n    Math comments are not working yet.  This is working now.\n    Still references are not linking to each other from notes to references. [1]_\n\n    $$\n    X(e^{j\\\\omega } ) = x(n)e^{ - j\\\\omega n}\n    $$\n\n    See Also\n    --------\n    test_newfunc : A variant of this function\n\n    References\n    ----------\n    .. [1] Author A, Author B, \"Title,\" Journal, volume, pages, year.\n\n\n    \"\"\"\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.build_2nd_order","title":"<code>build_2nd_order(p_conn_dist, dist_bins, **_)</code>","text":"<p>Build 2nd order model (exponential distance-dependent conn. prob.).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def build_2nd_order(p_conn_dist, dist_bins, **_):\n\"\"\"Build 2nd order model (exponential distance-dependent conn. prob.).\"\"\"\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n\n    exp_model = lambda x, a, b: a * np.exp(-b * np.array(x))\n    X = dist_bins[:-1][np.isfinite(p_conn_dist)] + bin_offset\n    y = p_conn_dist[np.isfinite(p_conn_dist)]\n    try:\n        (exp_model_scale, exp_model_exponent), _ = opt.curve_fit(exp_model, X, y, p0=[0.0, 0.0])\n    except:\n        logging.error(f'Exception while fitting model (\"{sys.exc_info()[1]}\")')\n        exp_model_scale = exp_model_exponent = np.nan\n\n    logging.info(f'MODEL FIT: f(x) = {exp_model_scale:.6f} * exp(-{exp_model_exponent:.6f} * x)')\n\n    model = 'exp_model_scale * np.exp(-exp_model_exponent * np.array(d))'\n    model_inputs = ['d']\n    model_params = {'exp_model_scale': exp_model_scale, 'exp_model_exponent': exp_model_exponent}\n\n    return {'model': model, 'model_inputs': model_inputs, 'model_params': model_params}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.build_3rd_order","title":"<code>build_3rd_order(p_conn_dist_bip, dist_bins, **_)</code>","text":"<p>Build 3rd order model (bipolar exp. distance-dependent conn. prob.).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def build_3rd_order(p_conn_dist_bip, dist_bins, **_):\n\"\"\"Build 3rd order model (bipolar exp. distance-dependent conn. prob.).\"\"\"\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n\n    X = dist_bins[:-1][np.all(np.isfinite(p_conn_dist_bip), 1)] + bin_offset\n    y = p_conn_dist_bip[np.all(np.isfinite(p_conn_dist_bip), 1), :]\n\n    exp_model = lambda x, a, b: a * np.exp(-b * np.array(x))\n    try:\n        (bip_neg_exp_model_scale, bip_neg_exp_model_exponent), _ = opt.curve_fit(exp_model, X, y[:, 0], p0=[0.0, 0.0])\n        (bip_pos_exp_model_scale, bip_pos_exp_model_exponent), _ = opt.curve_fit(exp_model, X, y[:, 1], p0=[0.0, 0.0])\n    except:\n        logging.error(f'Exception while fitting model (\"{sys.exc_info()[1]}\")')\n        bip_neg_exp_model_scale = bip_neg_exp_model_exponent = np.nan\n        bip_pos_exp_model_scale = bip_pos_exp_model_exponent = np.nan\n\n    logging.info(f'BIPOLAR MODEL FIT: f(x, dz) = {bip_neg_exp_model_scale:.6f} * exp(-{bip_neg_exp_model_exponent:.6f} * x) if dz &lt; 0')\n    logging.info(f'                              {bip_pos_exp_model_scale:.6f} * exp(-{bip_pos_exp_model_exponent:.6f} * x) if dz &gt; 0')\n    logging.info('                              AVERAGE OF BOTH MODELS  if dz == 0')\n\n    model = 'np.select([np.array(dz) &lt; 0, np.array(dz) &gt; 0, np.array(dz) == 0], [bip_neg_exp_model_scale * np.exp(-bip_neg_exp_model_exponent * np.array(d)), bip_pos_exp_model_scale * np.exp(-bip_pos_exp_model_exponent * np.array(d)), 0.5 * (bip_neg_exp_model_scale * np.exp(-bip_neg_exp_model_exponent * np.array(d)) + bip_pos_exp_model_scale * np.exp(-bip_pos_exp_model_exponent * np.array(d)))])'\n    model_inputs = ['d', 'dz']\n    model_params = {'bip_neg_exp_model_scale': bip_neg_exp_model_scale, 'bip_neg_exp_model_exponent': bip_neg_exp_model_exponent, 'bip_pos_exp_model_scale': bip_pos_exp_model_scale, 'bip_pos_exp_model_exponent': bip_pos_exp_model_exponent}\n\n    return {'model': model, 'model_inputs': model_inputs, 'model_params': model_params}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.compute_bip_matrix","title":"<code>compute_bip_matrix(src_depths, tgt_depths)</code>","text":"<p>Computes bipolar matrix between pairs of neurons based on depth difference delta_d:   POST-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) PRE-synaptic neuron</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def compute_bip_matrix(src_depths, tgt_depths):\n\"\"\"\n    Computes bipolar matrix between pairs of neurons based on depth difference delta_d:\n      POST-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) PRE-synaptic neuron\n    \"\"\"\n    bip_mat = np.sign(-np.diff(np.meshgrid(src_depths, tgt_depths, indexing='ij'), axis=0)[0, :, :])\n\n    return bip_mat\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.compute_dist_matrix","title":"<code>compute_dist_matrix(src_nrn_pos, tgt_nrn_pos)</code>","text":"<p>Computes distance matrix between pairs of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def compute_dist_matrix(src_nrn_pos, tgt_nrn_pos):\n\"\"\"Computes distance matrix between pairs of neurons.\"\"\"\n    dist_mat = spt.distance_matrix(src_nrn_pos, tgt_nrn_pos)\n    dist_mat[dist_mat == 0.0] = np.nan # Exclude autaptic connections\n\n    return dist_mat\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.compute_dist_matrix_symmetric","title":"<code>compute_dist_matrix_symmetric(nrn_pos)</code>","text":"<p>Computes symmetric distance matrix between pairs of neurons. Faster implementation to be used when source and target neurons are the same.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def compute_dist_matrix_symmetric(nrn_pos):\n\"\"\"Computes symmetric distance matrix between pairs of neurons.\n       Faster implementation to be used when source and target neurons\n       are the same.\"\"\"\n    dist_mat = spt.distance.squareform(spt.distance.pdist(nrn_pos))\n    dist_mat[dist_mat == 0.0] = np.nan # Exclude autaptic connections\n\n    return dist_mat\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_2nd_order_model","title":"<code>conn_prob_2nd_order_model(adj, node_properties, **kwargs)</code>","text":"<p>2nd-order probability model building, optionally for multiple random subsets of neurons.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>scipy.sparse</code> <p>Sparse adjacency matrix of the circuit</p> required <code>node_properties</code> <code>pandas.DataFrame</code> <p>Data frame with neuron properties</p> required <code>kwargs</code> <code>dict, optional</code> <p>Additional model building settings</p> <code>{}</code> <p>Returns:</p> Type Description <code>pandas.DataFrame</code> <p>Data frame with model paramters (columns) for different seeds (rows)</p>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_2nd_order_model--notes","title":"Notes","text":"<p>Description of 2nd-order model...</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_2nd_order_model(adj, node_properties, **kwargs):\n\"\"\"2nd-order probability model building, optionally for multiple random subsets of neurons.\n\n    Parameters\n    ----------\n    adj : scipy.sparse\n        Sparse adjacency matrix of the circuit\n    node_properties : pandas.DataFrame\n        Data frame with neuron properties\n    kwargs : dict, optional\n        Additional model building settings\n\n    Returns\n    -------\n    pandas.DataFrame\n        Data frame with model paramters (columns) for different seeds (rows)\n\n    Notes\n    -----\n    Description of 2nd-order model...\n    \"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_model(adj, node_properties, model_order=2, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_2nd_order_pathway_model","title":"<code>conn_prob_2nd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs)</code>","text":"<p>2nd-order probability model building for separate pathways (i.e., non-symmetric adj), optionally for multiple random subsets of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_2nd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs):\n\"\"\"2nd-order probability model building for separate pathways (i.e., non-symmetric adj),\n       optionally for multiple random subsets of neurons.\"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, model_order=2, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_3rd_order_model","title":"<code>conn_prob_3rd_order_model(adj, node_properties, **kwargs)</code>","text":"<p>3rd-order probability model building, optionally for multiple random subsets of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_3rd_order_model(adj, node_properties, **kwargs):\n\"\"\"3rd-order probability model building, optionally for multiple random subsets of neurons.\"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_model(adj, node_properties, model_order=3, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_3rd_order_pathway_model","title":"<code>conn_prob_3rd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs)</code>","text":"<p>3rd-order probability model building for separate pathways (i.e., non-symmetric adj), optionally for multiple random subsets of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_3rd_order_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs):\n\"\"\"3rd-order probability model building for separate pathways (i.e., non-symmetric adj),\n       optionally for multiple random subsets of neurons.\"\"\"\n\n    assert 'model_order' not in kwargs.keys(), f'ERROR: Invalid argument \"model_order\" in kwargs!'\n\n    return conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, model_order=3, **kwargs)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_model","title":"<code>conn_prob_model(adj, node_properties, **kwargs)</code>","text":"<p>General probability model building, optionally for multiple random subsets of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_model(adj, node_properties, **kwargs):\n\"\"\"General probability model building, optionally for multiple random subsets of neurons.\"\"\"\n\n    invalid_args = ['model_name', 'sample_seed'] # Not allowed arguments, as they will be set/used internally\n    for arg in invalid_args:\n        assert arg not in kwargs.keys(), f'ERROR: Invalid argument \"{arg}\" in kwargs!'\n    kwargs.update({'model_dir': None, 'data_dir': None, 'plot_dir': None, 'do_plot': False, 'N_split': None}) # Disable plotting/saving\n    model_name = None\n    model_order = kwargs.pop('model_order')\n\n    sample_size = kwargs.get('sample_size')\n    if sample_size is None or sample_size &gt;= node_properties.shape[0]:\n        sample_seeds = [None] # No randomization\n        if kwargs.pop('sample_seeds', None) is not None:\n            logging.warning('Using all neurons, ignoring sample seeds!')\n    else:\n        sample_seeds = kwargs.pop('sample_seeds', 1)\n\n        if not isinstance(sample_seeds, list): # sample_seeds corresponds to number of seeds to generate\n            sample_seeds = _generate_seeds(sample_seeds, meta_seed=kwargs.pop('meta_seed', 0))\n        else:\n            sample_seeds = list(np.unique(sample_seeds)) # Assure that unique and sorted\n\n    model_params = pd.DataFrame()\n    for seed in sample_seeds:\n        kwargs.update({'sample_seed': seed})\n        _, model_dict = run_model_building(adj, node_properties, model_name, model_order, **kwargs)\n        model_params = pd.concat([model_params, pd.DataFrame(model_dict['model_params'], index=pd.Index([seed], name='seed'))])\n\n    return model_params\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.conn_prob_pathway_model","title":"<code>conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs)</code>","text":"<p>General probability model building for separate pathways (i.e., non-symmetric adj), optionally for multiple random subsets of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def conn_prob_pathway_model(adj, node_properties_src, node_properties_tgt, **kwargs):\n\"\"\"General probability model building for separate pathways (i.e., non-symmetric adj),\n       optionally for multiple random subsets of neurons.\"\"\"\n\n    invalid_args = ['model_name', 'sample_seed'] # Not allowed arguments, as they will be set/used internally\n    for arg in invalid_args:\n        assert arg not in kwargs.keys(), f'ERROR: Invalid argument \"{arg}\" in kwargs!'\n    kwargs.update({'model_dir': None, 'data_dir': None, 'plot_dir': None, 'do_plot': False, 'N_split': None}) # Disable plotting/saving\n    model_name = None\n    model_order = kwargs.pop('model_order')\n\n    sample_size = kwargs.get('sample_size')\n    if sample_size is None or sample_size &gt;= np.maximum(node_properties_src.shape[0], node_properties_tgt.shape[0]):\n        sample_seeds = [None] # No randomization\n        if kwargs.pop('sample_seeds', None) is not None:\n            logging.warning('Using all neurons, ignoring sample seeds!')\n    else:\n        sample_seeds = kwargs.pop('sample_seeds', 1)\n\n        if not isinstance(sample_seeds, list): # sample_seeds corresponds to number of seeds to generate\n            sample_seeds = _generate_seeds(sample_seeds, meta_seed=kwargs.pop('meta_seed', 0))\n        else:\n            sample_seeds = list(np.unique(sample_seeds)) # Assure that unique and sorted\n\n    model_params = pd.DataFrame()\n    for seed in sample_seeds:\n        kwargs.update({'sample_seed': seed})\n        _, model_dict = run_pathway_model_building(adj, node_properties_src, node_properties_tgt, model_name, model_order, **kwargs)\n        model_params = pd.concat([model_params, pd.DataFrame(model_dict['model_params'], index=pd.Index([seed], name='seed'))])\n\n    return model_params\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.extract_2nd_order","title":"<code>extract_2nd_order(adj, node_properties, bin_size_um = 100, max_range_um = None, coord_names = None, split_indices = None, part_idx = None, **_)</code>","text":"<p>Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def extract_2nd_order(adj, node_properties, bin_size_um=100, max_range_um=None, coord_names=None, split_indices=None, part_idx=None, **_):\n\"\"\"Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons.\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n    if isinstance(split_indices, list):\n        N_split = len(split_indices)\n    else:\n        N_split = 0 # Don't split\n    if part_idx is not None: # Run only data extraction of given part idx\n        assert 0 &lt;= part_idx &lt; N_split, 'ERROR: Part index out of range!'\n\n    pos_table = node_properties[coord_names].to_numpy()\n\n    if N_split == 0: # Compute all at once\n        # Compute distance matrix\n        dist_mat = compute_dist_matrix_symmetric(pos_table)\n\n        # Extract distance-dependent connection probabilities\n        if max_range_um is None:\n            max_range_um = np.nanmax(dist_mat)\n        num_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_bins + 1) * bin_size_um\n\n        p_conn_dist, count_conn, count_all = extract_dependent_p_conn(adj, [dist_mat], [dist_bins])\n\n    else: # Split computation into N_split data splits (to reduce memory consumption)\n        assert max_range_um is not None, f'ERROR: Max. range must be specified if data extraction splitted into {N_split} parts!'\n        num_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_bins + 1) * bin_size_um\n\n        count_conn = np.zeros(num_bins, dtype=int)\n        count_all = np.zeros(num_bins, dtype=int)\n        for sidx, split_sel in enumerate(split_indices):\n            if part_idx is not None and part_idx != sidx:\n                continue\n            logging.info(f'&lt;SPLIT {sidx + 1} of {N_split}&gt;')\n\n            # Compute distance matrix\n            dist_mat_split = compute_dist_matrix(pos_table[split_sel, :], pos_table)\n\n            # Extract distance-dependent connection counts\n            _, count_conn_split, count_all_split = extract_dependent_p_conn(adj[split_sel, :], [dist_mat_split], [dist_bins])\n            count_conn += count_conn_split\n            count_all += count_all_split\n\n        # Compute overall connection probabilities\n        p_conn_dist = np.array(count_conn / count_all)\n#         p_conn_dist[np.isnan(p_conn_dist)] = 0.0\n\n    return {'p_conn_dist': p_conn_dist, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.extract_2nd_order_pathway","title":"<code>extract_2nd_order_pathway(adj, node_properties_src, node_properties_tgt, bin_size_um = 100, max_range_um = None, coord_names = None, split_indices = None, part_idx = None, **_)</code>","text":"<p>Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons for separate pathways (i.e., non-symmetric adj).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def extract_2nd_order_pathway(adj, node_properties_src, node_properties_tgt, bin_size_um=100, max_range_um=None, coord_names=None, split_indices=None, part_idx=None, **_):\n\"\"\"Extract distance-dependent connection probability (2nd order) from a sample of pairs of neurons\n       for separate pathways (i.e., non-symmetric adj).\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n\n    assert split_indices is None and part_idx is None, 'ERROR: Data splitting not supported!'\n\n    pos_table_src = node_properties_src[coord_names].to_numpy()\n    pos_table_tgt = node_properties_tgt[coord_names].to_numpy()\n\n    # Compute distance matrix\n    dist_mat = compute_dist_matrix(pos_table_src, pos_table_tgt)\n\n    # Extract distance-dependent connection probabilities\n    if max_range_um is None:\n        max_range_um = np.nanmax(dist_mat)\n    num_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n    dist_bins = np.arange(0, num_bins + 1) * bin_size_um\n\n    p_conn_dist, count_conn, count_all = extract_dependent_p_conn(adj, [dist_mat], [dist_bins])\n\n    return {'p_conn_dist': p_conn_dist, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.extract_3rd_order","title":"<code>extract_3rd_order(adj, node_properties, bin_size_um = 100, max_range_um = None, coord_names = None, depth_name = None, split_indices = None, part_idx = None, **_)</code>","text":"<p>Extract distance-dependent connection probability (3rd order) from a sample of pairs of neurons.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def extract_3rd_order(adj, node_properties, bin_size_um=100, max_range_um=None, coord_names=None, depth_name=None, split_indices=None, part_idx=None, **_):\n\"\"\"Extract distance-dependent connection probability (3rd order) from a sample of pairs of neurons.\"\"\"\n\n    if coord_names is None:\n        coord_names = ['x', 'y', 'z'] # Default names of coordinatate system axes as in node_properties\n    if depth_name is None:\n        depth_name = 'depth' # Default name of depth column in node_properties\n    if isinstance(split_indices, list):\n        N_split = len(split_indices)\n    else:\n        N_split = 0 # Don't split\n    if part_idx is not None: # Run only data extraction of given part idx\n        assert 0 &lt;= part_idx &lt; N_split, 'ERROR: Part index out of range!'\n\n    pos_table = node_properties[coord_names].to_numpy()\n    depth_table = node_properties[depth_name].to_numpy()\n\n    if N_split == 0: # Compute all at once\n        # Compute distance matrix\n        dist_mat = compute_dist_matrix_symmetric(pos_table)\n\n        # Compute bipolar matrix (post-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) pre-synaptic neuron)\n        bip_mat = compute_bip_matrix(depth_table, depth_table)\n\n        # Extract bipolar distance-dependent connection probabilities\n        if max_range_um is None:\n            max_range_um = np.nanmax(dist_mat)\n        num_dist_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_dist_bins + 1) * bin_size_um\n        bip_bins = [np.nanmin(bip_mat), 0, np.nanmax(bip_mat)]\n\n        p_conn_dist_bip, count_conn, count_all = extract_dependent_p_conn(adj, [dist_mat, bip_mat], [dist_bins, bip_bins])\n\n    else: # Split computation into N_split data splits (to reduce memory consumption)\n        assert max_range_um is not None, f'ERROR: Max. range must be specified if data extraction splitted into {N_split} parts!'\n        num_dist_bins = np.ceil(max_range_um / bin_size_um).astype(int)\n        dist_bins = np.arange(0, num_dist_bins + 1) * bin_size_um\n        bip_bins = [-1, 0, 1]\n\n        count_conn = np.zeros([num_dist_bins, 2], dtype=int)\n        count_all = np.zeros([num_dist_bins, 2], dtype=int)\n        for sidx, split_sel in enumerate(split_indices):\n            if part_idx is not None and part_idx != sidx:\n                continue\n            logging.info(f'&lt;SPLIT {sidx + 1} of {N_split}&gt;')\n\n            # Compute distance matrix\n            dist_mat_split = compute_dist_matrix(pos_table[split_sel, :], pos_table)\n\n            # Compute bipolar matrix (post-synaptic neuron below (delta_d &lt; 0) or above (delta_d &gt; 0) pre-synaptic neuron)\n            bip_mat_split = compute_bip_matrix(depth_table[split_sel], depth_table)\n\n            # Extract distance-dependent connection counts\n            _, count_conn_split, count_all_split = extract_dependent_p_conn(adj[split_sel, :], [dist_mat_split, bip_mat_split], [dist_bins, bip_bins])\n            count_conn += count_conn_split\n            count_all += count_all_split\n\n        # Compute overall connection probabilities\n        p_conn_dist_bip = np.array(count_conn / count_all)\n#         p_conn_dist_bip[np.isnan(p_conn_dist_bip)] = 0.0\n\n    return {'p_conn_dist_bip': p_conn_dist_bip, 'count_conn': count_conn, 'count_all': count_all, 'dist_bins': dist_bins, 'bip_bins': bip_bins}\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.extract_dependent_p_conn","title":"<code>extract_dependent_p_conn(adj, dep_matrices, dep_bins)</code>","text":"<p>Extract D-dimensional conn. prob. dependent on D property matrices between source-target pairs of neurons within given range of bins.</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def extract_dependent_p_conn(adj, dep_matrices, dep_bins):\n\"\"\"Extract D-dimensional conn. prob. dependent on D property matrices between source-target pairs of neurons within given range of bins.\"\"\"\n    num_dep = len(dep_matrices)\n    assert len(dep_bins) == num_dep, 'ERROR: Dependencies/bins mismatch!'\n    assert np.all([dep_matrices[dim].shape == adj.shape for dim in range(num_dep)]), 'ERROR: Matrix dimension mismatch!'\n\n    # Extract connection probability\n    num_bins = [len(b) - 1 for b in dep_bins]\n    bin_indices = [list(range(n)) for n in num_bins]\n    count_all = np.full(num_bins, -1) # Count of all pairs of neurons for each combination of dependencies\n    count_conn = np.full(num_bins, -1) # Count of connected pairs of neurons for each combination of dependencies\n\n    logging.info(f'Extracting {num_dep}-dimensional ({\"x\".join([str(n) for n in num_bins])}) connection probabilities...')\n    pbar = progressbar.ProgressBar(maxval=np.prod(num_bins) - 1)\n    for idx in pbar(itertools.product(*bin_indices)):\n        dep_sel = np.full(adj.shape, True)\n        for dim in range(num_dep):\n            lower = dep_bins[dim][idx[dim]]\n            upper = dep_bins[dim][idx[dim] + 1]\n            dep_sel = np.logical_and(dep_sel, np.logical_and(dep_matrices[dim] &gt;= lower, (dep_matrices[dim] &lt; upper) if idx[dim] &lt; num_bins[dim] - 1 else (dep_matrices[dim] &lt;= upper))) # Including last edge\n        sidx, tidx = np.nonzero(dep_sel)\n        count_all[idx] = np.sum(dep_sel)\n        ### count_conn[idx] = np.sum(adj[sidx, tidx]) # ERROR in scipy/sparse/compressed.py if len(sidx) &gt;= 2**31: \"ValueError: could not convert integer scalar\"\n        # [WORKAROUND]: Split indices into parts of 2**31-1 length and sum them separately\n        sidx_split = np.split(sidx, np.arange(0, len(sidx), 2**31-1)[1:])\n        tidx_split = np.split(tidx, np.arange(0, len(tidx), 2**31-1)[1:])\n        count_split = 0\n        for s, t in zip(sidx_split, tidx_split):\n            count_split = count_split + np.sum(adj[s, t])\n        count_conn[idx] = count_split\n    p_conn = np.array(count_conn / count_all)\n#     p_conn[np.isnan(p_conn)] = 0.0\n\n    return p_conn, count_conn, count_all\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.get_model_function","title":"<code>get_model_function(model, model_inputs, model_params)</code>","text":"<p>Returns model function from string representation [so any model function can be saved to file].</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def get_model_function(model, model_inputs, model_params):\n\"\"\"Returns model function from string representation [so any model function can be saved to file].\"\"\"\n    input_str = ','.join(model_inputs + ['model_params=model_params']) # String representation of input variables\n    input_param_str = ','.join(model_inputs + list(model_params.keys())) # String representation of input variables and model parameters\n    model_param_str = ','.join(model_inputs + ['**model_params']) # String representation propagating model parameters\n\n    inner_model_str = f'lambda {input_param_str}: {model}'\n    full_model_str = f'lambda {input_str}: ({inner_model_str})({model_param_str})' # Use nested lambdas to bind local variables\n\n    model_fct = eval(full_model_str) # Build function\n\n    # logging.info(f'Model function: {inner_model_str}')\n\n    return model_fct\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.plot_2nd_order","title":"<code>plot_2nd_order(adj, node_properties, model_name, p_conn_dist, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir = None, **_)</code>","text":"<p>Visualize data vs. model (2nd order).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def plot_2nd_order(adj, node_properties, model_name, p_conn_dist, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir=None, **_):\n\"\"\"Visualize data vs. model (2nd order).\"\"\"\n    if plot_dir is not None:\n        if not os.path.exists(plot_dir):\n            os.makedirs(plot_dir)\n\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n    dist_model = np.linspace(dist_bins[0], dist_bins[-1], 100)\n\n    model_str = f'f(x) = {model_params[\"exp_model_scale\"]:.3f} * exp(-{model_params[\"exp_model_exponent\"]:.3f} * x)'\n    model_fct = get_model_function(model, model_inputs, model_params)\n\n    plt.figure(figsize=(12, 4), dpi=300)\n\n    # Data vs. model\n    plt.subplot(1, 2, 1)\n    plt.step(dist_bins, np.hstack([p_conn_dist[0], p_conn_dist]), color=DATA_COLOR, label=f'Data: N = {node_properties.shape[0]}x{node_properties.shape[0]} cells')\n    plt.plot(dist_bins[:-1] + bin_offset, p_conn_dist, '.', color=DATA_COLOR)\n    plt.plot(dist_model, model_fct(dist_model), '--', color=MODEL_COLOR, label='Model: ' + model_str)\n    plt.grid()\n    plt.xlabel('Distance ($\\\\mu$m)')\n    plt.ylabel('Conn. prob.')\n    plt.title('Data vs. model fit')\n    plt.legend()\n\n    # 2D connection probability (model)\n    plt.subplot(1, 2, 2)\n    plot_range = 500 # (um)\n    r_markers = [200, 400] # (um)\n    dx = np.linspace(-plot_range, plot_range, 201)\n    dz = np.linspace(plot_range, -plot_range, 201)\n    xv, zv = np.meshgrid(dx, dz)\n    vdist = np.sqrt(xv**2 + zv**2)\n    pdist = model_fct(vdist)\n    plt.imshow(pdist, interpolation='bilinear', extent=(-plot_range, plot_range, -plot_range, plot_range), cmap=PROB_CMAP, vmin=0.0)\n    for r in r_markers:\n        plt.gca().add_patch(plt.Circle((0, 0), r, edgecolor='w', linestyle='--', fill=False))\n        plt.text(0, r, f'{r} $\\\\mu$m', color='w', ha='center', va='bottom')\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel('$\\\\Delta$x')\n    plt.ylabel('$\\\\Delta$z')\n    plt.title('2D model')\n    plt.colorbar(label='Conn. prob.')\n\n    plt.suptitle(f'Distance-dependent connection probability model (2nd order)')\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_vs_model.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n\n    # Data counts\n    plt.figure(figsize=(12, 4), dpi=300)\n    plt.bar(dist_bins[:-1] + bin_offset, count_all, width=2.0 * bin_offset, edgecolor='k', label='All pair count')\n    plt.bar(dist_bins[:-1] + bin_offset, count_conn, width=1.5 * bin_offset, label='Connection count')\n    plt.gca().set_yscale('log')\n    plt.xticks(dist_bins, rotation=45)\n    plt.grid()\n    plt.xlabel('Distance ($\\\\mu$m)')\n    plt.ylabel('Count')\n    plt.title(f'Distance-dependent connection counts (N = {node_properties.shape[0]}x{node_properties.shape[0]} cells)')\n    plt.legend()\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_counts.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.plot_3rd_order","title":"<code>plot_3rd_order(adj, node_properties, model_name, p_conn_dist_bip, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir = None, **_)</code>","text":"<p>Visualize data vs. model (3rd order).</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def plot_3rd_order(adj, node_properties, model_name, p_conn_dist_bip, count_conn, count_all, dist_bins, model, model_inputs, model_params, plot_dir=None, **_):\n\"\"\"Visualize data vs. model (3rd order).\"\"\"\n    if plot_dir is not None:\n        if not os.path.exists(plot_dir):\n            os.makedirs(plot_dir)\n\n    bin_offset = 0.5 * np.diff(dist_bins[:2])[0]\n    dist_model = np.linspace(dist_bins[0], dist_bins[-1], 100)\n\n    model_strN = f'{model_params[\"bip_neg_exp_model_scale\"]:.3f} * exp(-{model_params[\"bip_neg_exp_model_exponent\"]:.3f} * x)'\n    model_strP = f'{model_params[\"bip_pos_exp_model_scale\"]:.3f} * exp(-{model_params[\"bip_pos_exp_model_exponent\"]:.3f} * x)'\n    model_fct = get_model_function(model, model_inputs, model_params)\n\n    plt.figure(figsize=(12, 4), dpi=300)\n\n    # Data vs. model\n    plt.subplot(1, 2, 1)\n    bip_dist = np.concatenate((-dist_bins[:-1][::-1] - bin_offset, [0.0], dist_bins[:-1] + bin_offset))\n    bip_data = np.concatenate((p_conn_dist_bip[::-1, 0], [np.nan], p_conn_dist_bip[:, 1]))\n    all_bins = np.concatenate((-dist_bins[1:][::-1], [0.0], dist_bins[1:]))\n    bin_data = np.concatenate((p_conn_dist_bip[::-1, 0], p_conn_dist_bip[:, 1]))\n    plt.step(all_bins, np.hstack([bin_data[0], bin_data]), color=DATA_COLOR, label=f'Data: N = {node_properties.shape[0]}x{node_properties.shape[0]} cells')\n    plt.plot(bip_dist, bip_data, '.', color=DATA_COLOR)\n    plt.plot(-dist_model, model_fct(dist_model, np.sign(-dist_model)), '--', color=MODEL_COLOR, label='Model: ' + model_strN)\n    plt.plot(dist_model, model_fct(dist_model, np.sign(dist_model)), '--', color=MODEL_COLOR2, label='Model: ' + model_strP)\n    plt.grid()\n    plt.xlabel('sign($\\\\Delta$z) * Distance [$\\\\mu$m]')\n    plt.ylabel('Conn. prob.')\n    plt.title('Data vs. model fit')\n    plt.legend(loc='upper left', fontsize=8)\n\n    # 2D connection probability (model)\n    plt.subplot(1, 2, 2)\n    plot_range = 500 # (um)\n    r_markers = [200, 400] # (um)\n    dx = np.linspace(-plot_range, plot_range, 201)\n    dz = np.linspace(plot_range, -plot_range, 201)\n    xv, zv = np.meshgrid(dx, dz)\n    vdist = np.sqrt(xv**2 + zv**2)\n    pdist = model_fct(vdist, np.sign(zv))\n    plt.imshow(pdist, interpolation='bilinear', extent=(-plot_range, plot_range, -plot_range, plot_range), cmap=PROB_CMAP, vmin=0.0)\n    plt.plot(plt.xlim(), np.zeros(2), 'w', linewidth=0.5)\n    for r in r_markers:\n        plt.gca().add_patch(plt.Circle((0, 0), r, edgecolor='w', linestyle='--', fill=False))\n        plt.text(0, r, f'{r} $\\\\mu$m', color='w', ha='center', va='bottom')\n    plt.xticks([])\n    plt.yticks([])\n    plt.xlabel('$\\\\Delta$x')\n    plt.ylabel('$\\\\Delta$z')\n    plt.title('2D model')\n    plt.colorbar(label='Conn. prob.')\n\n    plt.suptitle(f'Bipolar distance-dependent connection probability model (3rd order)')\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_vs_model.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n\n    # Data counts\n    bip_count = np.concatenate((count_conn[::-1, 0], [np.nan], count_conn[:, 1]))\n    bip_count_all = np.concatenate((count_all[::-1, 0], [np.nan], count_all[:, 1]))\n    plt.figure(figsize=(12, 4), dpi=300)\n    plt.bar(bip_dist, bip_count_all, width=2.0 * bin_offset, edgecolor='k', label='All pair count')\n    plt.bar(bip_dist, bip_count, width=1.5 * bin_offset, label='Connection count')\n    plt.gca().set_yscale('log')\n    plt.grid()\n    plt.xlabel('sign($\\\\Delta$z) * Distance [$\\\\mu$m]')\n    plt.ylabel('Count')\n    plt.title(f'Bipolar distance-dependent connection counts (N = {node_properties.shape[0]}x{node_properties.shape[0]} cells)')\n    plt.legend()\n    plt.tight_layout()\n    if plot_dir is not None:\n        out_fn = os.path.abspath(os.path.join(plot_dir, model_name + '__data_counts.png'))\n        plt.savefig(out_fn)\n        logging.info(f'Figure saved to {out_fn}')\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_batch_model_building","title":"<code>run_batch_model_building(adj_file, nrn_file, cfg_file, N_split = None, part_idx = None)</code>","text":"<p>Main function for data extraction and model building to be used in a batch script on different data splits.</p> <p>Parameters:</p> Name Type Description Default <code>adj_file</code> <code>str</code> <p>File name (.npz format) of scipy.sparse adjacency matrix of the circuit</p> required <code>nrn_file</code> <code>str</code> <p>File name (.h5 or .feather format) of pandas.DataFrame with neuron properties</p> required <code>cfg_file</code> <code>str</code> <p>File name (.json format) of config dict specifying the model building operation</p> required <code>N_split</code> <code>int, optional</code> <p>Number of data splits to divide data extraction into (to reduce memory consumption)</p> <code>None</code> <code>part_idx</code> <code>int, optional</code> <p>Index of current data split (part) to extract data from Range: 0 .. N_split - 1 Run data extraction of given data split -1               Merge data splits and build model</p> <code>None</code> <p>Returns:</p> Type Description <code>None</code> <p>Nothing returned here; Data/model/figures are written to disc</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If nrn_file is not in .h5 or .feather format</p> <code>AssertionError</code> <p>If the adjacency matrix is not a square matrix matching the length of the neuron properties table</p>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_batch_model_building--see-also","title":"See Also","text":"<p>run_model_building : Underlying main function for model building</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def run_batch_model_building(adj_file, nrn_file, cfg_file, N_split=None, part_idx=None):\n\"\"\"Main function for data extraction and model building to be used in a batch script on different data splits.\n\n    Parameters\n    ----------\n    adj_file : str\n        File name (.npz format) of scipy.sparse adjacency matrix of the circuit\n    nrn_file : str\n        File name (.h5 or .feather format) of pandas.DataFrame with neuron properties\n    cfg_file : str\n        File name (.json format) of config dict specifying the model building operation\n    N_split : int, optional\n        Number of data splits to divide data extraction into (to reduce memory consumption)\n    part_idx : int, optional\n        Index of current data split (part) to extract data from\n        Range: 0 .. N_split - 1 Run data extraction of given data split\n               -1               Merge data splits and build model\n\n    Returns\n    -------\n    None\n        Nothing returned here; Data/model/figures are written to disc\n\n    Raises\n    ------\n    AssertionError\n        If nrn_file is not in .h5 or .feather format\n    AssertionError\n        If the adjacency matrix is not a square matrix matching the length of the neuron properties table\n\n    See Also\n    --------\n    run_model_building : Underlying main function for model building\n    \"\"\"\n\n    # Load adjacency matrix (.npz) &amp; neuron properties table (.h5 or .feather)\n    adj = sps.load_npz(adj_file)\n    if os.path.splitext(nrn_file)[-1] == '.h5':\n        node_properties = pd.read_hdf(nrn_file)\n    elif os.path.splitext(nrn_file)[-1] == '.feather':\n        node_properties = pd.read_feather(nrn_file)\n    else:\n        assert False, f'ERROR: Neuron table format \"{os.path.splitext(nrn_file)[-1]}\" not supported!'\n\n    assert adj.shape[0] == adj.shape[1] == node_properties.shape[0], 'ERROR: Data size mismatch!'\n    logging.info(f'Loaded connectivity and properties of {node_properties.shape[0]} neurons')\n\n    # Load config file (.json)\n    with open(cfg_file, 'r') as f:\n        config_dict = json.load(f)\n\n    # Set/Overwrite data split options\n    if N_split is not None:\n        config_dict.update({'N_split': int(N_split)})\n    if part_idx is not None:\n        config_dict.update({'part_idx': int(part_idx)})\n\n    # Run model building\n    run_model_building(adj, node_properties, **config_dict)\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_model_building","title":"<code>run_model_building(adj, node_properties, model_name, model_order, **kwargs)</code>","text":"<p>Main function for running model building, consisting of three steps:   Data extraction, model fitting, and (optionally) data/model visualization</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def run_model_building(adj, node_properties, model_name, model_order, **kwargs):\n\"\"\"\n    Main function for running model building, consisting of three steps:\n      Data extraction, model fitting, and (optionally) data/model visualization\n    \"\"\"\n    logging.info(f'Running order-{model_order} model building {kwargs}...')\n\n    # Subsampling (optional)\n    sample_size = kwargs.get('sample_size')\n    sample_seed = kwargs.get('sample_seed')\n    if sample_size is not None and sample_size &gt; 0 and sample_size &lt; node_properties.shape[0]:\n        logging.info(f'Subsampling to {sample_size} of {node_properties.shape[0]} neurons (seed={sample_seed})')\n        np.random.seed(sample_seed)\n        sub_sel = np.random.permutation([True] * sample_size + [False] * (node_properties.shape[0] - sample_size))\n        adj = adj.tocsr()[sub_sel, :].tocsc()[:, sub_sel].tocsr()\n        node_properties = node_properties.loc[sub_sel, :]\n\n    # Set modelling functions\n    if model_order == 2: # Distance-dependent\n        fct_extract = extract_2nd_order\n        fct_fit = build_2nd_order\n        fct_plot = plot_2nd_order\n    elif model_order == 3: # Bipolar distance-dependent\n        fct_extract = extract_3rd_order\n        fct_fit = build_3rd_order\n        fct_plot = plot_3rd_order\n    else:\n        assert False, f'ERROR: Order-{model_order} model building not supported!'\n\n    # Data splits (optional)\n    N_split = kwargs.pop('N_split', None)\n    part_idx = kwargs.pop('part_idx', None)\n    if N_split is None:\n        split_indices = None\n    else:\n        assert N_split &gt; 1, 'ERROR: Number of data splits must be larger than 1!'\n        split_indices = np.split(np.arange(node_properties.shape[0]), np.cumsum([np.ceil(node_properties.shape[0] / N_split).astype(int)] * (N_split - 1)))\n\n    if part_idx is None or part_idx == -1: # Run data extraction and model building for all splits\n        extract_only = False\n        data_fn = 'data'\n    else: # Run only data extraction of given part idx\n        assert N_split is not None and 0 &lt;= part_idx &lt; N_split, 'ERROR: Part index out of range!'\n        extract_only = True\n        data_fn = 'data' + get_data_part_name(N_split, part_idx)\n\n    # Extract connection probability data\n    if part_idx == -1: # Special case: Load and merge results of existing parts\n        assert N_split is not None, 'ERROR: Number of data splits required!'\n        data_dict = merge_data(kwargs.get('data_dir'), model_name, data_fn, [get_data_part_name(N_split, p) for p in range(N_split)])\n    else:\n        data_dict = fct_extract(adj, node_properties, split_indices=split_indices, part_idx=part_idx, **kwargs)\n    save_data(data_dict, kwargs.get('data_dir'), model_name, data_fn)\n\n    if extract_only: # Stop here and return data dict\n        return data_dict, {}\n\n    # Fit model\n    model_dict = fct_fit(**data_dict, **kwargs)\n    save_data(model_dict, kwargs.get('model_dir'), model_name, 'model')\n\n    # Visualize data/model (optional)\n    if kwargs.get('do_plot'):\n        fct_plot(adj, node_properties, model_name, **data_dict, **model_dict, **kwargs)\n\n    return data_dict, model_dict\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.run_pathway_model_building","title":"<code>run_pathway_model_building(adj, node_properties_src, node_properties_tgt, model_name, model_order, **kwargs)</code>","text":"<p>Main function for running model building for separate pathways (i.e., non-symmetric adj),   consisting of three steps: Data extraction, model fitting, and (optionally) data/model visualization</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def run_pathway_model_building(adj, node_properties_src, node_properties_tgt, model_name, model_order, **kwargs):\n\"\"\"\n    Main function for running model building for separate pathways (i.e., non-symmetric adj),\n      consisting of three steps: Data extraction, model fitting, and (optionally) data/model visualization\n    \"\"\"\n    logging.info(f'Running order-{model_order} model building {kwargs}...')\n\n    # Subsampling (optional)\n    sample_size = kwargs.get('sample_size')\n    sample_seed = kwargs.get('sample_seed')\n    if sample_size is not None and sample_size &gt; 0 and sample_size &lt; np.maximum(node_properties_src.shape[0], node_properties_tgt.shape[0]):\n        logging.info(f'Subsampling to {sample_size} of {node_properties_src.shape[0]}x{node_properties_tgt.shape[0]} neurons (seed={sample_seed})')\n        np.random.seed(sample_seed)\n        if sample_size &lt; node_properties_src.shape[0]:\n            sub_sel_src = np.random.permutation([True] * sample_size + [False] * (node_properties_src.shape[0] - sample_size))\n        else:\n            sub_sel_src = np.full(node_properties_src.shape[0], True)\n\n        if sample_size &lt; node_properties_tgt.shape[0]:\n            sub_sel_tgt = np.random.permutation([True] * sample_size + [False] * (node_properties_tgt.shape[0] - sample_size))\n        else:\n            sub_sel_tgt = np.full(node_properties_tgt.shape[0], True)\n\n        adj = adj.tocsr()[sub_sel_src, :].tocsc()[:, sub_sel_tgt].tocsr()\n        # adj = adj[sub_sel_src, :][:, sub_sel_tgt]\n        node_properties_src = node_properties_src.loc[sub_sel_src, :]\n        node_properties_tgt = node_properties_tgt.loc[sub_sel_tgt, :]\n\n    # Set modelling functions\n    if model_order == 2: # Distance-dependent\n        fct_extract = extract_2nd_order_pathway\n        fct_fit = build_2nd_order\n        fct_plot = plot_2nd_order\n    else:\n        assert False, f'ERROR: Order-{model_order} model building not supported!'\n\n    # Data splits (optional)\n    N_split = kwargs.pop('N_split', None)\n    part_idx = kwargs.pop('part_idx', None)\n    assert N_split is None and part_idx is None, 'ERROR: Data splitting not supported!'\n    data_fn = 'data'\n\n    # Extract connection probability data\n    data_dict = fct_extract(adj, node_properties_src, node_properties_tgt, split_indices=None, part_idx=None, **kwargs)\n    save_data(data_dict, kwargs.get('data_dir'), model_name, data_fn)\n\n    # Fit model\n    model_dict = fct_fit(**data_dict, **kwargs)\n    save_data(model_dict, kwargs.get('model_dir'), model_name, 'model')\n\n    # Visualize data/model (optional)\n    if kwargs.get('do_plot'):\n        assert False, 'ERROR: Plotting not supported!'\n\n    return data_dict, model_dict\n</code></pre>"},{"location":"modelling/#src.connalysis.modelling.modelling.save_data","title":"<code>save_data(save_dict, save_dir, model_name, save_spec = None)</code>","text":"<p>Writes data/model dict to pickled data file</p> Source code in <code>src/connalysis/modelling/modelling.py</code> <pre><code>def save_data(save_dict, save_dir, model_name, save_spec=None):\n\"\"\"Writes data/model dict to pickled data file\"\"\"\n    if not save_dir:\n        return\n\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    if save_spec is None:\n        save_spec = ''\n    else:\n        save_spec = '__' + save_spec\n\n    save_file = os.path.join(save_dir, f'{model_name}{save_spec}.pickle')\n    with open(save_file, 'wb') as f:\n        pickle.dump(save_dict, f)\n\n    logging.info(f'Pickled dict written to {save_file}')\n</code></pre>"},{"location":"network/","title":"Functions for working with networks","text":"<p>This page describes functions contained in the <code>network</code> module.</p>"},{"location":"network/#src.connalysis.network.topology.at_weight_edges","title":"<code>at_weight_edges(weighted_adj, threshold, method = 'strength')</code>","text":"<p>Returns thresholded network on edges :param method: distance returns edges with weight smaller or equal than thresh                strength returns edges with weight larger or equal than thresh                assumes csr format for weighted_adj</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def at_weight_edges(weighted_adj, threshold, method=\"strength\"):\n\"\"\" Returns thresholded network on edges\n    :param method: distance returns edges with weight smaller or equal than thresh\n                   strength returns edges with weight larger or equal than thresh\n                   assumes csr format for weighted_adj\"\"\"\n    data=weighted_adj.data\n    data_thresh=np.zeros(data.shape)\n    if method == \"strength\":\n        data_thresh[data&gt;=threshold]=data[data&gt;=threshold]\n    elif method == \"distance\":\n        data_thresh[data&lt;=threshold]=data[data&lt;=threshold]\n    else:\n        raise ValueError(\"Method has to be 'strength' or 'distance'\")\n    adj_thresh=weighted_adj.copy()\n    adj_thresh.data=data_thresh\n    adj_thresh.eliminate_zeros()\n    return adj_thresh\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.bedge_counts","title":"<code>bedge_counts(adjacency, simplices = None, max_simplices = False, max_dim = -1, simplex_type = 'directed', **kwargs)</code>","text":"<p>Count the sum number of edges per position on the subgraphs defined by the nodes of the simplices in simplices.</p> <p>Parameters:</p> Name Type Description Default <code>adjacency</code> <code>(N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>simplices</code> <code>series</code> <p>Series  of 2d-arrays indexed by dimension. Each array is of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <p>See simplex_counts</p> <code>'directed'</code> <p>Returns:</p> Type Description <code>series</code> <p>pandas series with index dimensions values (dim+1, dim+1) arrays.  The (i,j) entry counts the number of edges from node i to node j on all the subgraphs of adjacency on the nodes of the simplices listed.  See notes.</p>"},{"location":"network/#src.connalysis.network.topology.bedge_counts--notes","title":"Notes","text":"<p>Every directed \\(k\\)-simplex \\([v_o, v_1, \\ldots, v_k]\\) defines as subgraph of the adjacency matrix, with edges \\(v_i \\to v_j\\) whenever \\(i\\leq j\\), but also possibly with ''reverse'' edges.  One can represent this structure with a non-symmetric \\((k+1, k+1)\\)-matrix with <code>1</code>'s for every edge in the subgraph.  The output of this function gives for each dimension the sum of all these matrices over all the simplices provided in <code>simplices</code> or over all the simplices in the adjacency matrix if none is provided.  The lower triangular part of these matrices is therefore a metric of recurrence within simplices, or \"higher dimensional recurrence\". In particular, in dimension 1 it is the number of reciprocal edges in the network.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def bedge_counts(adjacency, simplices=None,\n                 max_simplices = False, max_dim = -1, simplex_type = 'directed', ** kwargs):\n\"\"\"Count the sum number of edges per position on the subgraphs defined by the nodes of the simplices in simplices.\n\n        Parameters\n        ----------\n        adjacency : (N,N)-array or sparse matrix\n            Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n            The matrix can be asymmetric, but must have 0 in the diagonal.\n        simplices : series\n            Series  of 2d-arrays indexed by dimension.\n            Each array is of dimension (no. of simplices, dimension).\n            Each row corresponds to a list of nodes on a simplex.\n        max_simplices : bool\n            If False counts all simplices in adj.\n            If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n        max_dim : int\n            Maximal dimension up to which simplex motifs are counted.\n            The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n        simplex_type: str\n            See [simplex_counts](network.md#src.connalysis.network.topology.simplex_counts)\n\n        Returns\n        -------\n        series\n            pandas series with index dimensions values (dim+1, dim+1) arrays.  The (i,j) entry counts the number of edges\n            from node i to node j on all the subgraphs of adjacency on the nodes of the simplices listed.  See notes.\n\n        Notes\n        -------\n        Every directed $k$-simplex $[v_o, v_1, \\\\ldots, v_k]$ defines as subgraph of the adjacency matrix, with edges\n        $v_i \\\\to v_j$ whenever $i\\leq j$, but also possibly with ''reverse'' edges.  One can represent this structure\n        with a non-symmetric $(k+1, k+1)$-matrix with `1`'s for every edge in the subgraph.  The output of this function\n        gives for each dimension the sum of all these matrices over all the simplices provided in `simplices` or over\n        all the simplices in the adjacency matrix if none is provided.  The lower triangular part of these matrices is\n        therefore a metric of recurrence within simplices, or \"higher dimensional recurrence\".\n        In particular, in dimension 1 it is the number of reciprocal edges in the network.\n        \"\"\"\n\n    adj = adjacency\n\n    if simplices is None:\n        LOG.info(\"COMPUTE `bedge_counts(...)`: No argued simplices.\")\n        return bedge_counts(adj,\n                            list_simplices_by_dimension(adj, max_simplices = max_simplices,\n                                                        max_dim = max_dim, simplex_type = simplex_type, ** kwargs))\n    else:\n        LOG.info(\"COMPUTE `bedge_counts(...): for simplices: %s \", simplices.shape)\n\n    dense = np.array(adjacency.toarray(), dtype=int)\n\n    def subset_adj(simplex):\n        return dense[simplex].T[simplex]\n\n    def count_bedges(simplices_given_dim):\n\"\"\"...\"\"\"\n        try:\n            d_simplices = simplices_given_dim.get_value()\n        except AttributeError:\n            d_simplices = simplices_given_dim\n\n        if d_simplices is None or d_simplices.shape[1] == 1:\n            return np.nan\n\n        return (pd.DataFrame(d_simplices, columns=range(d_simplices.shape[1]))\n                .apply(subset_adj, axis=1)\n                .agg(\"sum\"))\n\n    return simplices.apply(count_bedges)\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.betti_counts","title":"<code>betti_counts(adj, node_properties = None, min_dim = 0, max_dim = [], simplex_type = 'directed', approximation = None, **kwargs)</code>","text":"<p>Count betti counts of flag complex of adj.  Type of flag complex is given by simplex_type.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d (N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.  Matrix will be cast to 0,1 entries so weights will be ignored.</p> required <code>node_properties</code> <code> data frame</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>min_dim</code> <code>int</code> <p>Minimal dimension from which betti counts are computed. The default min_dim = 0 (counting number of connected components).</p> <code>0</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = [] counts betti numbers up to the maximal dimension of the complex.</p> <code>[]</code> <code>simplex_type</code> <code>string</code> <p>Type of flag complex to consider, given by the type of simplices it is built on. Possible types are:</p> <p>\u2019directed\u2019 - directed simplices (directed flag complex)</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph (clique complex of the underlying undirected graph)</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections (clique complex of the undirected graph of reciprocal connections.)</p> <code>'directed'</code> <code>approximation</code> <code>list of integers  or None</code> <p>Approximation parameter for the computation of the betti numbers.  Useful for large networks. If None all betti numbers are computed exactly. Otherwise, min_dim must be 0 and approximation but be a list of positive integers or -1. The list approximation is either extended by -1 entries on the right or sliced from [0:max_dim+1] to obtain a list of length max_dim.  Each entry of the list denotes the approximation value for the betti computation of that dimension if -1 approximation in that dimension is set to None.</p> <p>If the approximation value at a given dimension is <code>a</code> flagser skips cells creating columns in the reduction matrix with more than <code>a</code> entries.  This is useful for hard problems.  For large, sparse networks a good value if often <code>100,00</code>.  If set to <code>1</code> that dimension will be virtually ignored.  See [1]_</p> <code>None</code> <p>Returns:</p> Type Description <code>series</code> <p>Betti counts indexed per dimension from min_dim to max_dim.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> <code>AssertionError</code> <p>If approximation != None and min_dim != 0.</p>"},{"location":"network/#src.connalysis.network.topology.betti_counts--see-also","title":"See Also","text":"<p>simplex_counts : A function that counts the simplices forming the complex from which bettis are count. Simplex types are described there in detail.</p>"},{"location":"network/#src.connalysis.network.topology.betti_counts--references","title":"References","text":"<p>For details about the approximation algorithm see</p> <p>..[1] D. Luetgehetmann, \"Documentation of the C++ flagser library\";        GitHub: luetge/flagser.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def betti_counts(adj, node_properties=None,\n                 min_dim=0, max_dim=[], simplex_type='directed', approximation=None,\n                 **kwargs):\n\"\"\"Count betti counts of flag complex of adj.  Type of flag complex is given by simplex_type.\n\n    Parameters\n    ----------\n    adj : 2d (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.  Matrix will be cast to 0,1 entries so weights\n        will be ignored.\n    node_properties :  data frame\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    min_dim : int\n        Minimal dimension from which betti counts are computed.\n        The default min_dim = 0 (counting number of connected components).\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = [] counts betti numbers up to the maximal dimension of the complex.\n    simplex_type : string\n        Type of flag complex to consider, given by the type of simplices it is built on.\n        Possible types are:\n\n        \u2019directed\u2019 - directed simplices (directed flag complex)\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph (clique complex of the underlying undirected graph)\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections (clique complex of the\n        undirected graph of reciprocal connections.)\n    approximation : list of integers  or None\n        Approximation parameter for the computation of the betti numbers.  Useful for large networks.\n        If None all betti numbers are computed exactly.\n        Otherwise, min_dim must be 0 and approximation but be a list of positive integers or -1.\n        The list approximation is either extended by -1 entries on the right or sliced from [0:max_dim+1] to obtain\n        a list of length max_dim.  Each entry of the list denotes the approximation value for the betti computation\n        of that dimension if -1 approximation in that dimension is set to None.\n\n        If the approximation value at a given dimension is `a` flagser skips cells creating columns in the reduction\n        matrix with more than `a` entries.  This is useful for hard problems.  For large, sparse networks a good value\n        if often `100,00`.  If set to `1` that dimension will be virtually ignored.  See [1]_\n\n    Returns\n    -------\n    series\n        Betti counts indexed per dimension from min_dim to max_dim.\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    AssertionError\n        If approximation != None and min_dim != 0.\n\n    See Also\n    --------\n    [simplex_counts](network.md#src.connalysis.network.topology.simplex_counts) :\n    A function that counts the simplices forming the complex from which bettis are count.\n    Simplex types are described there in detail.\n\n    References\n    ----------\n    For details about the approximation algorithm see\n\n    ..[1] D. Luetgehetmann, \"Documentation of the C++ flagser library\";\n           [GitHub: luetge/flagser](https://github.com/luetge/flagser/blob/master/docs/documentation_flagser.pdf).\n\n    \"\"\"\n    LOG.info(\"Compute betti counts for %s-type adjacency matrix and %s-type node properties\",\n             type(adj), type(node_properties))\n\n    from pyflagser import flagser_unweighted\n\n    #Checking matrix\n    adj = sp.csr_matrix(adj).astype(bool).astype('int')\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n    assert not((not approximation is None) and (min_dim!=0)), \\\n        'For approximation != None, min_dim must be set to 0.  \\nLower dimensions can be ignored by setting approximation to 1 on those dimensions'\n\n    # Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type == 'undirected':\n        adj = sp.triu(underlying_undirected_matrix(adj))  # symmtrize and keep upper triangular only\n    elif simplex_type == \"reciprocal\":\n        adj = sp.triu(rc_submatrix(adj))  # symmtrize and keep upper triangular only\n    #Computing bettis\n    if max_dim==[]:\n        max_dim=np.inf\n\n    if approximation==None:\n        LOG.info(\"Run without approximation\")\n        bettis = flagser_unweighted(adj, min_dimension=min_dim, max_dimension=max_dim,\n                                    directed=True, coeff=2,\n                                    approximation=None)['betti']\n    else:\n        assert (all([isinstance(item,int) for item in approximation])) # assert it's a list of integers\n        approximation=np.array(approximation)\n        bettis=[]\n\n        #Make approximation vector to be of size max_dim\n        if max_dim!=np.inf:\n            if approximation.size-1 &lt; max_dim:#Vector too short so pad with -1's\n                approximation=np.pad(approximation,\n                                     (0,max_dim-(approximation.size-1)),\n                                     'constant',constant_values=-1)\n            if approximation.size-1&gt;max_dim:#Vector too long, select relevant slice\n                approximation=approximation[0:max_dim+1]\n            #Sanity check\n            LOG.info(\"Correct dimensions for approximation: %s\", approximation.size==max_dim+1)\n\n        #Split approximation into sub-vectors of same value to speed up computation\n        diff=approximation[1:]-approximation[:-1]\n        slice_indx=np.array(np.where(diff!=0)[0])+1\n\n        #Compute betti counts\n        for dims_range in  np.split(np.arange(approximation.size),slice_indx):\n            n=dims_range[0] #min dim for computation\n            N=dims_range[-1] #max dim for computation\n            a=approximation[n]\n            if a==-1:\n                a=None\n            LOG.info(\"Run betti for dim range %s-%s with approximation %s\", n,N,a)\n            bettis=bettis+flagser_unweighted(adj, min_dimension=n, max_dimension=N,\n                                             directed=True, coeff=2,\n                                             approximation=a)['betti']\n\n        if max_dim==np.inf:\n            n=approximation.size #min dim for computation\n            N=np.inf #max dim for computation\n            a=None\n            LOG.info(\"Run betti for dim range %s-%s with approximation %s\",n,N,a)\n            bettis=bettis+flagser_unweighted(adj, min_dimension=n, max_dimension=N,\n                                             directed=True, coeff=2,\n                                             approximation=a)['betti']\n\n    return pd.Series(bettis, name=\"betti_count\",\n                     index=pd.Index(np.arange(min_dim, len(bettis)+min_dim), name=\"dim\"))\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.bin_weigths","title":"<code>bin_weigths(weights, n_bins = 10, return_bins = False)</code>","text":"<p>Bins the np.array weights Input: np.array of floats, no of bins returns: bins, and binned data i.e. a np.array of the same shape as weights with entries the center value of its corresponding bin</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def bin_weigths(weights, n_bins=10, return_bins=False):\n'''Bins the np.array weights\n    Input: np.array of floats, no of bins\n    returns: bins, and binned data i.e. a np.array of the same shape as weights with entries the center value of its corresponding bin\n    '''\n    tol = 1e-8 #to include the max value in the last bin\n    min_weight = weights.min()\n    max_weight = weights.max() + tol\n    step = (max_weight - min_weight) / n_bins\n    bins = np.arange(min_weight, max_weight + step, step)\n    digits = np.digitize(weights, bins)\n\n    weights = (min_weight + step / 2) + (digits - 1) * step\n    return (weights, bins) if return_bins else weights\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.count_rc_edges_k_skeleton","title":"<code>count_rc_edges_k_skeleton(simplex_list_at_dim, N, position = 'all', return_mat = False)</code>","text":"<p>Count the edges and reciprocal edges in the simplex list provided. If the list is all the k (maximal)simplices of a directed flag complex, it is counting the number of edges and reciprocal edges of the its k-skeleton.</p> <p>Parameters:</p> Name Type Description Default <code>simplex_list_at_dim</code> <p>Array of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex indexed by the ordering given for all nodes in the graph</p> required <code>N</code> <p>Number of nodes in original graph</p> required <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex</p> <p>'spine': edges along the spine of the simplex (only makes sense for directed simplices)</p> <code>'all'</code> <p>Returns:</p> Type Description <code>tuple of ints</code> <p>Counts of (edges, reciprocal edges) in the simplex list</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If N &lt;= than an entry in the simplex list</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def count_rc_edges_k_skeleton(simplex_list_at_dim, N, position=\"all\", return_mat=False):\n\"\"\"Count the edges and reciprocal edges in the simplex list provided.\n    If the list is all the k (maximal)simplices of a directed flag complex, it is counting the number of\n    edges and reciprocal edges of the its k-skeleton.\n\n    Parameters\n    ----------\n    simplex_list_at_dim: 2d-array\n        Array of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex indexed by the\n        ordering given for all nodes in the graph\n    N: int\n        Number of nodes in original graph\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex\n\n        'spine': edges along the spine of the simplex\n        (only makes sense for directed simplices)\n\n    Returns\n    -------\n    tuple of ints\n        Counts of (edges, reciprocal edges) in the simplex list\n\n    Raises\n    ------\n    AssertionError\n        If N &lt;= than an entry in the simplex list\n    \"\"\"\n\n    assert N &gt; np.max(simplex_list_at_dim), \\\n        \"N must be larger than all the entries in the simplex list\"\n\n    mat = extract_submatrix_of_simplices(simplex_list_at_dim, N, position=position)\n    edge_counts = mat.sum()\n    rc_edge_counts = rc_submatrix(mat).sum()\n    # Return mats?\n    if return_mat == True:\n        return edge_counts, rc_edge_counts, mat\n    else:\n        return edge_counts, rc_edge_counts\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.count_rc_edges_skeleta","title":"<code>count_rc_edges_skeleta(adj = None, max_dim = -1, max_simplices = False, simplex_list = None, N = None, position = 'all', return_mats = False, **kwargs)</code>","text":"<p>Count the edges and reciprocal edges in the k-skeleta of the directed flag complex of adj for all k&lt;= max_dim. If simplex list are provided, it will compute the skeleta directly from these and not use adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>(N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex</code> <p>Series 2d-arrays indexed by dimension. Each array is of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex. If provided adj will be ignored but N will be required.</p> required <code>N</code> <p>Number of nodes in original graph.</p> <code>None</code> <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex</p> <p>'spine': edges along the spine of the simplex (only makes sense if simplices are directed)</p> <code>'all'</code> <code>return_mats</code> <code>bool</code> <p>If True return the matrices of the underlying graphs of the k-skeleta as in get_k_skeleta_graph.</p> <code>False</code> <p>Returns:</p> Type Description <code>data frame, (dict)</code> <p>data frame with index dimensions and columns number of (rc) edges in the corresponding skeleta if return_mats==True, also return the graphs of the k-skeleta as in get_k_skeleta_graph.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If neither adj nor simplex_list are provided</p> <code>AssertionError</code> <p>If N &lt;= than an entry in the simplex list</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def count_rc_edges_skeleta(adj=None, max_dim=-1, max_simplices=False,\n                           simplex_list=None, N=None,\n                           position=\"all\", return_mats=False, **kwargs):\n    # check max dim is consistent with simplex_list only used if adj is given and must be &gt;0\n\"\"\"Count the edges and reciprocal edges in the k-skeleta of the directed flag complex of adj for all\n    k&lt;= max_dim. If simplex list are provided, it will compute the skeleta directly from these and not use adj.\n\n    Parameters\n    ----------\n    adj : (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex list: series\n        Series 2d-arrays indexed by dimension.\n        Each array is of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex.\n        If provided adj will be ignored but N will be required.\n    N: int\n        Number of nodes in original graph.\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex\n\n        'spine': edges along the spine of the simplex\n        (only makes sense if simplices are directed)\n    return_mats : bool\n        If True return the matrices of the underlying graphs of the k-skeleta as in\n        get_k_skeleta_graph.\n\n    Returns\n    -------\n    data frame, (dict)\n        data frame with index dimensions and columns number of (rc) edges in the corresponding skeleta\n        if return_mats==True, also return the graphs of the k-skeleta as in get_k_skeleta_graph.\n\n    Raises\n    ------\n    AssertionError\n        If neither adj nor simplex_list are provided\n    AssertionError\n        If N &lt;= than an entry in the simplex list\n    \"\"\"\n\n    assert not (adj is None and simplex_list is None), \"Either adj or simplex_list need to be provided\"\n\n    if (simplex_list is None):  # Compute simplex lists if not provided\n        simplex_list = list_simplices_by_dimension(adj, node_properties=None,\n                                                   max_simplices=max_simplices, max_dim=max_dim,\n                                                   simplex_type='directed',\n                                                   nodes=None, verbose=False, **kwargs)\n        N = adj.shape[0]\n    else:\n        assert N &gt; np.nanmax(simplex_list.explode().explode()), \\\n            \"N must be larger than all the entries in the simplex list\"\n\n    # Extract 'k'-skeleton and count (rc-)edges\n    dims = simplex_list.index[simplex_list.index != 0]  # Doesn't make sense to look at the 0-skeleton\n    edge_counts = pd.DataFrame(index=dims, columns=[\"number_of_edges\", \"number_of_rc_edges\", \"rc/edges_percent\"])\n    if return_mats == True:\n        skeleton_mats = {f'dimension_{dim}': None for dim in dims}\n    for dim in dims:\n        edges, rc_edges, mat = count_rc_edges_k_skeleton(simplex_list[dim], N,\n                                                         position=position, return_mat=True)\n        edge_counts[\"number_of_edges\"].loc[dim] = edges\n        edge_counts[\"number_of_rc_edges\"].loc[dim] = rc_edges\n        edge_counts[\"rc/edges_percent\"].loc[dim] = (rc_edges * 100) / edges\n        if return_mats == True:\n            skeleton_mats[f'dimension_{dim}'] = mat\n    if return_mats == True:\n        return edge_counts, skeleton_mats\n    else:\n        return edge_counts\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.count_triads_fully_connected","title":"<code>count_triads_fully_connected(adj, max_num_sampled = 5000000)</code>","text":"<p>Counts the numbers of each triadic motif in the matrix adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d-array</code> <p>Adjacency matrix of a directed network.</p> required <code>max_num_sampled</code> <code>int</code> <p>The maximal number of connected triads classified. If the number of connected triads is higher than that, only the specified number is sampled at random and classified. The final counts are extrapolated as (actual_num_triads/ max_num_sampled) * counts.</p> <code>5000000</code> <p>Returns:</p> Type Description <code>1d array</code> <p>The counts of the various triadic motifs in adj as ordered in Figure 5 [1]_.</p>"},{"location":"network/#src.connalysis.network.topology.count_triads_fully_connected--notes","title":"Notes","text":"<p>Only connectected motifs are counted, i.e. motifs with less than 2 connections or only a single bidirectional connection are not counted. The connected motifs are ordered as in Figure 5 [1]_.</p>"},{"location":"network/#src.connalysis.network.topology.count_triads_fully_connected--references","title":"References","text":"<p>..[1] Gal, Eyal, et al. \"Rich cell-type-specific network topology in neocortical microcircuitry.\" Nature neuroscience 20.7 (2017): 1004-1013.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def count_triads_fully_connected(adj, max_num_sampled=5000000):\n\"\"\"Counts the numbers of each triadic motif in the matrix adj.\n\n    Parameters\n    ----------\n    adj : 2d-array\n        Adjacency matrix of a directed network.\n    max_num_sampled : int\n        The maximal number of connected triads classified. If the number of\n        connected triads is higher than that, only the specified number is sampled at random and\n        classified. The final counts are extrapolated as (actual_num_triads/ max_num_sampled) * counts.\n\n    Returns\n    -------\n    1d array\n        The counts of the various triadic motifs in adj as ordered in Figure 5 [1]_.\n\n    Notes\n    ------\n    Only connectected motifs are counted, i.e. motifs with less than 2 connections or only a single bidirectional\n    connection are not counted. The connected motifs are ordered as in Figure 5 [1]_.\n\n    References\n    -------\n\n    ..[1] Gal, Eyal, et al.\n    [\"Rich cell-type-specific network topology in neocortical microcircuitry.\"](https://www.nature.com/articles/nn.4576)\n    Nature neuroscience 20.7 (2017): 1004-1013.\n\n    \"\"\"\n\n    # Functions to indetify triads\n    def canonical_sort(M):\n\"\"\"Sorts row/columns of the matrix adj using the lexicographical order of the\n        tuple (out_degree, in_degree).\n\n        Parameters\n        ----------\n        M : 2d-array\n            Adjacency matrix of a directed network.\n\n        Returns\n        -------\n        2d-array\n            the matrix adj with rows/columns sorted\n        \"\"\"\n        in_degree = np.sum(M, axis=0)\n        out_degree = np.sum(M, axis=1)\n        idx = np.argsort(-in_degree - 10 * out_degree)\n        return M[:, idx][idx]\n\n    def identify_motif(M):\n\"\"\"\n        Identifies the connected directed digraph on three nodes M as on in the full classification\n        list given in the dictionary triad_dict.\n\n        Parameters\n        ----------\n        M : array\n            A (3,3) array describing a directed connected digraph on three nodes.\n\n        Returns\n        -------\n        The index of the motif as indexed in the dictiroanry triad_dict which follows the\n        ordering of Gal et al., 2017\n        \"\"\"\n        triad_code = tuple(np.nonzero(canonical_sort(M).flatten())[0])\n        return triad_dict[triad_code]\n\n    # Finding and counting triads\n    import time\n    adj = adj.toarray()  # Casting to array makes finding triads an order of magnitude faster\n    t0 = time.time()\n    undirected_adj = underlying_undirected_matrix(adj).toarray()\n    # Matrix with i,j entries number of undirected paths between i and j in adj\n    path_counts = np.triu(undirected_adj @ undirected_adj, 1)\n    connected_pairs = np.nonzero(path_counts)\n    triads = set()\n    print(\"Testing {0} potential triadic pairs\".format(len(connected_pairs[0])))\n    for x, y in zip(*connected_pairs):\n        # zs = np.nonzero((undirected_adj.getrow(x).multiply(undirected_adj.getrow(y))).toarray()[0])[0]\n        zs = np.nonzero(undirected_adj[x] &amp; undirected_adj[y])[0]\n        for z in zs:\n            triads.add(tuple(sorted([x, y, z])))\n    triads = list(triads)\n    print(\"Time spent finding triads: {0}\".format(time.time() - t0))\n    print(\"Found {0} connected triads\".format(len(triads)))\n    t0 = time.time()\n    counts = np.zeros(np.max(list(triad_dict.values())) + 1)\n    sample_idx = np.random.choice(len(triads),\n                                  np.minimum(max_num_sampled, len(triads)),\n                                  replace=False)\n    for idx in sample_idx:\n        triad = triads[idx]\n        motif_id = identify_motif(adj[:, triad][triad, :])\n        counts[motif_id] += 1\n    print(\"Time spent classifying triads: {0}\".format(time.time() - t0))\n    return ((len(triads) / len(sample_idx)) * counts).astype(int)\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.cross_col_k_in_degree","title":"<code>cross_col_k_in_degree(adj_cross, adj_source, max_simplices = False, threads = 1, max_dim = -1, **kwargs)</code>","text":"<p>Compute generalized in-degree of nodes in adj_target from nodes in adj_source. The k-in-degree of a node v is the number of k-simplices in adj_source with all its nodes mapping to v through edges in adj_cross.</p> <p>Parameters:</p> Name Type Description Default <code>adj_cross</code> <code>(n,m) array or sparse matrix</code> <p>Matrix of connections from the nodes in adj_n to the target population. n is the number of nodes in adj_source and m is the number of nodes in adj_target. A non-zero entry adj_cross[i,j] implies there is an edge from i-th node of adj_source to the j-th node of adj_target.</p> required <code>adj_source</code> <code>(n, n)-array or sparse matrix</code> <p>Adjacency matrix of the source network where n is the number of nodes in the source network. A non-zero entry adj_source[i,j] implies there is an edge from node i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices. If True counts only maximal simplices.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions. Particularly useful for large or dense graphs.</p> <code>-1</code> <p>Returns:</p> Type Description <code>Data frame</code> <p>Table of cross-k-in-degrees indexed by the m nodes in the target population.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj_source has non-zero entries in the diagonal which can produce errors.</p>"},{"location":"network/#src.connalysis.network.topology.cross_col_k_in_degree--notes","title":"Notes","text":"<p>We should probably write some notes here</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def cross_col_k_in_degree(adj_cross, adj_source, max_simplices=False,\n                          threads=1,max_dim=-1,**kwargs):\n    #TODO DO THE OUTDEGREE VERSION maybe one where populations are defined within a matrix?\n\"\"\"Compute generalized in-degree of nodes in adj_target from nodes in adj_source.\n    The k-in-degree of a node v is the number of k-simplices in adj_source with all its nodes mapping to v\n    through edges in adj_cross.\n    Parameters\n    ----------\n    adj_cross : (n,m) array or sparse matrix\n        Matrix of connections from the nodes in adj_n to the target population.\n        n is the number of nodes in adj_source and m is the number of nodes in adj_target.\n        A non-zero entry adj_cross[i,j] implies there is an edge from i-th node of adj_source\n        to the j-th node of adj_target.\n    adj_source : (n, n)-array or sparse matrix\n        Adjacency matrix of the source network where n is the number of nodes in the source network.\n        A non-zero entry adj_source[i,j] implies there is an edge from node i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    max_simplices : bool\n        If False counts all simplices.\n        If True counts only maximal simplices.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.\n        Particularly useful for large or dense graphs.\n\n    Returns\n    -------\n    Data frame\n        Table of cross-k-in-degrees indexed by the m nodes in the target population.\n\n    Raises\n    ------\n    AssertionError\n        If adj_source has non-zero entries in the diagonal which can produce errors.\n\n    Notes\n    -----\n    We should probably write some notes here\n    \"\"\"\n    adj_source=sp.csr_matrix(adj_source).astype('bool')\n    adj_cross=sp.csr_matrix(adj_cross).astype('bool')\n    assert np.count_nonzero(adj_source.diagonal()) == 0, \\\n    'The diagonal of the source matrix is non-zero and this may lead to errors!'\n    assert adj_source.shape[0] == adj_source.shape[1], \\\n    'Dimension mismatch. The source matrix must be square.'\n    assert adj_source.shape[0] == adj_cross.shape[0], \\\n    'Dimension mismatch. The source matrix and cross matrix must have the same number of rows.'\n\n    n_source = adj_source.shape[0] #Size of the source population\n    n_target = adj_cross.shape[1] #Size of the target population\n    # Building a square matrix [[adj_source, adj_cross], [0,0]]\n    adj=sp.bmat([[adj_source, adj_cross],\n                 [sp.csr_matrix((n_target, n_source), dtype='bool'),\n                  sp.csr_matrix((n_target, n_target), dtype='bool')]])\n    # Transposing to restrict computation to ``source nodes'' in adj_target in flagsercount\n    adj=adj.T\n    nodes=np.arange(n_source, n_source+n_target) #nodes on target population\n    slist=list_simplices_by_dimension(adj, max_simplices=max_simplices, max_dim=max_dim,nodes=nodes,\n                                      simplex_type='directed',verbose=False,threads=threads,**kwargs)\n\n    #Count participation as a source in transposed matrix i.e. participation as sink in the original\n    cross_col_deg=pd.DataFrame(columns=slist.index[1:], index=nodes)\n    for dim in slist.index[1:]:\n        index,deg=np.unique(slist[dim][:,0],return_counts=True)\n        cross_col_deg[dim].loc[index]=deg\n    cross_col_deg=cross_col_deg.fillna(0)\n    return cross_col_deg\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.extract_submatrix_of_simplices","title":"<code>extract_submatrix_of_simplices(simplex_list, N, position = 'all')</code>","text":"<p>Generate binary submatrix of NxN matrix of edges in simplex list.</p> <p>Parameters:</p> Name Type Description Default <code>simplex</code> <p>Array of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex indexed by the order of the nodes in an NxN matrix.</p> required <code>N</code> <p>Number of nodes in original graph defining the NxN matrix.</p> required <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex</p> <p>'spine': edges along the spine of the simplex (only makes sense for directed simplices)</p> <code>'all'</code> <p>Returns:</p> Type Description <code>csr bool matrix</code> <p>Matrix with of shape (N,N) with entries <code>True</code> corresponding to edges in simplices.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def extract_submatrix_of_simplices(simplex_list, N, position=\"all\"):\n\"\"\"Generate binary submatrix of NxN matrix of edges in simplex list.\n\n    Parameters\n    ----------\n    simplex list: 2d-array\n        Array of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex\n        indexed by the order of the nodes in an NxN matrix.\n    N: int\n        Number of nodes in original graph defining the NxN matrix.\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex\n\n        'spine': edges along the spine of the simplex\n        (only makes sense for directed simplices)\n\n    Returns\n    -------\n    csr bool matrix\n        Matrix with of shape (N,N) with entries `True` corresponding to edges in simplices.\n    \"\"\"\n    if simplex_list.shape[0] == 0:\n        return sp.csr_matrix((N, N), dtype=bool)  # no simplices in this dimension\n    else:\n        dim = simplex_list.shape[1] - 1\n        edges_abstract = _generate_abstract_edges_in_simplices(dim,\n                                                               position=position)  # abstract list of edges to extract from each simplex\n        edges = np.unique(np.concatenate([simplex_list[:, edge] for edge in edges_abstract]), axis=0)\n        return (sp.coo_matrix((np.ones(edges.shape[0]), (edges[:, 0], edges[:, 1])), shape=(N, N))).tocsr().astype(bool)\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.filtered_simplex_counts","title":"<code>filtered_simplex_counts(adj, node_properties = None, method = 'strength', binned = False, n_bins = 10, threads = 1, **kwargs)</code>","text":"<p>Takes weighted adjancecy matrix returns data frame with filtered simplex counts where index is the weight method strength higher weights enter first, method distance smaller weights enter first</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def filtered_simplex_counts(adj, node_properties=None, method=\"strength\",\n                            binned=False, n_bins=10, threads=1,\n                            **kwargs):\n'''Takes weighted adjancecy matrix returns data frame with filtered simplex counts where index is the weight\n    method strength higher weights enter first, method distance smaller weights enter first'''\n    from tqdm import tqdm\n    adj = adj.copy()\n    if binned==True:\n        adj.data = bin_weigths(adj.data, n_bins=n_bins)\n\n    weights = filtration_weights(adj, node_properties, method)\n\n#    TODO: 1. Prove that the following is executed in the implementation that follows.\n#    TODO: 2. If any difference, update the implementation\n#    TODO: 3. Remove the reference code.\n#    n_simplices = dict.fromkeys(weights)\n#    for weight in tqdm(weights[::-1],total=len(weights)):\n#        adj = at_weight_edges(adj, threshold=weight, method=method)\n#        n_simplices[weight] = simplex_counts(adj, threads=threads)\n\n    m = method\n    def filter_weight(w):\n        adj_w = at_weight_edges(adj, threshold=w, method=m)\n        return simplex_counts(adj_w, threads=threads)\n\n    n_simplices = {w: filter_weight(w) for w in weights[::-1]}\n    return pd.DataFrame.from_dict(n_simplices, orient=\"index\").fillna(0).astype(int)\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.filtration_weights","title":"<code>filtration_weights(adj, node_properties = None, method = 'strength')</code>","text":"<p>Returns the filtration weights of a given weighted adjacency matrix. :param method: distance smaller weights enter the filtration first                strength larger weights enter the filtration first</p> <p>TODO: Should there be a warning when the return is an empty array because the matrix is zero?</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def filtration_weights(adj, node_properties=None, method=\"strength\"):\n\"\"\"\n    Returns the filtration weights of a given weighted adjacency matrix.\n    :param method: distance smaller weights enter the filtration first\n                   strength larger weights enter the filtration first\n\n    TODO: Should there be a warning when the return is an empty array because the matrix is zero?\n    \"\"\"\n    if method == \"strength\":\n        return np.unique(adj.data)[::-1]\n\n    if method == \"distance\":\n        return np.unique(adj.data)\n\n    raise ValueError(\"Method has to be 'strength' or 'distance'\")\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.get_all_simplices_from_max","title":"<code>get_all_simplices_from_max(max_simplices)</code>","text":"<p>Takes the list of maximal simplices are returns the list of all simplices.</p> <p>Parameters:</p> Name Type Description Default <code>max_simplices</code> <code>list</code> <p>A list of lists of tuples. Where max_simplices[k] is a list of the 0 dimensional maximal simplices, where each simplex is a tuple of the vertices of the simplex</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of lists of tuples. Of the same format as the inputted list but now contains all simplices.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def get_all_simplices_from_max(max_simplices):\n\"\"\"Takes the list of maximal simplices are returns the list of all simplices.\n\n        Parameters\n        ----------\n        max_simplices : list\n            A list of lists of tuples. Where max_simplices[k] is a list of the 0 dimensional maximal simplices,\n            where each simplex is a tuple of the vertices of the simplex\n\n        Returns\n        -------\n        list\n            A list of lists of tuples. Of the same format as the inputted list but now contains all simplices.\n        \"\"\"\n    simplices = list(max_simplices)\n    for k in range(len(max_simplices)-1,0,-1):\n        print(max_simplices[k])\n        for simplex in simplices[k]:\n            for s in range(k,-1,-1):\n                x = tuple(simplex[:s]+simplex[s+1:])\n                if x not in simplices[k-1]:\n                    simplices[k-1].append(x)\n\n    return simplices\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.get_k_skeleta_graph","title":"<code>get_k_skeleta_graph(adj = None, max_simplices = False, dimensions = None, simplex_type = 'directed', simplex_list = None, N = None, position = 'all', **kwargs)</code>","text":"<p>Return the edges of the (maximal) k-skeleton of the flag complex of adj for all k&lt;= max_dim in the position determined by position. If simplex list are provided, it will compute the edges directly from these and not use adj, in which case N (the number of rows and columns of adj) is required. If simplex lists are not provided they will be calculated with for the flag complex whose type is determined by simplex_type as for simplex_counts.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>(N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>dimensions</code> <code>list of ints</code> <p>Dimensions <code>k</code> for which the <code>k</code>-skeleta is computed, if None all dimensions are computed.</p> <code>None</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider if computed from adj:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <code>simplex</code> <p>Series 2d-arrays indexed by dimension. Each array is of dimension (no. of simplices, dimension). Each row corresponds to a list of nodes on a simplex. If provided adj will be ignored but N will be required.</p> required <code>N</code> <p>Number of nodes in original graph.</p> <code>None</code> <code>position</code> <p>Position of the edges to extract</p> <p>'all': all edges of the simplex</p> <p>'spine': edges along the spine of the simplex (only makes sense if simplices are directed)</p> <code>'all'</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary with keys dimensions and values boolean (N,N) matrices with entries <code>True</code> corresponding to edges in (maximal) simplices of that dimension.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If neither adj nor simplex_list are provided</p> <code>AssertionError</code> <p>If N &lt;= than an entry in the simplex list</p> <code>AssertionError</code> <p>If a dimension is required that is not an index in the simplex list</p>"},{"location":"network/#src.connalysis.network.topology.get_k_skeleta_graph--notes","title":"Notes","text":"<p>In order to list k-simplices and thus the k-skeleton, flagsercount needs to list all lower dimensional simplices anyhow.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def get_k_skeleta_graph(adj=None, max_simplices=False, dimensions=None, simplex_type='directed',\n                        simplex_list=None, N=None, position=\"all\",\n                        **kwargs):\n    # Choose only some dimensions???\n    # check max dim is consistent with simplex_list only used if adj is given and must be &gt;0\n    # adj only used is simplex list is none\n    # Add requirement to give adj is direction is undirected and multiply adj by mat!!!\n\"\"\"Return the edges of the (maximal) k-skeleton of the flag complex of adj for all k&lt;= max_dim in the position determined\n    by position.\n    If simplex list are provided, it will compute the edges directly from these and not use adj,\n    in which case N (the number of rows and columns of adj) is required.\n    If simplex lists are not provided they will be calculated with for the flag complex whose type is determined by\n    simplex_type as for simplex_counts.\n\n    Parameters\n    ----------\n    adj : (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    dimensions : list of ints\n        Dimensions `k` for which the `k`-skeleta is computed, if None all dimensions are computed.\n    simplex_type : string\n        Type of simplex to consider if computed from adj:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n    simplex list: series\n        Series 2d-arrays indexed by dimension.\n        Each array is of dimension (no. of simplices, dimension).\n        Each row corresponds to a list of nodes on a simplex.\n        If provided adj will be ignored but N will be required.\n    N: int\n        Number of nodes in original graph.\n    position: str\n        Position of the edges to extract\n\n        'all': all edges of the simplex\n\n        'spine': edges along the spine of the simplex\n        (only makes sense if simplices are directed)\n\n    Returns\n    -------\n    dict\n        Dictionary with keys dimensions and values boolean (N,N) matrices with entries `True`\n        corresponding to edges in (maximal) simplices of that dimension.\n\n    Raises\n    ------\n    AssertionError\n        If neither adj nor simplex_list are provided\n    AssertionError\n        If N &lt;= than an entry in the simplex list\n    AssertionError\n        If a dimension is required that is not an index in the simplex list\n\n    Notes\n    ------\n    In order to list k-simplices and thus the k-skeleton, flagsercount needs to list all lower\n    dimensional simplices anyhow.\n\n    \"\"\"\n\n    assert not (adj is None and simplex_list is None), \"Either adj or simplex_list need to be provided\"\n\n    if dimensions == None:\n        max_dim = -1\n    else:\n        max_dim = np.max(np.array(dimensions))\n\n    if (simplex_list is None):  # Compute simplex lists if not provided\n        simplex_list = list_simplices_by_dimension(adj, node_properties=None,\n                                                   max_simplices=max_simplices, max_dim=max_dim,\n                                                   simplex_type=simplex_type,\n                                                   nodes=None, verbose=False, **kwargs)\n        N = adj.shape[0]\n    else:\n        assert isinstance(N, int), 'If simplex list are provide N must be provided'\n        assert N &gt; np.nanmax(simplex_list.explode().explode()), \\\n            \"N must be larger than all the entries in the simplex list\"\n        assert (dimensions == None) or np.isin(dimensions, simplex_list.index).all(), \\\n            f'Some requested dimensions={dimensions} are not in the simplex lists index={simplex_list.index.to_numpy()}'\n    # Extract 'k'-skeleton\n    dims = simplex_list.index[simplex_list.index != 0]  # Doesn't make sense to look at the 0-skeleton\n    if dimensions != None:\n        dims = dims[np.isin(dims, dimensions)]\n    skeleton_mats = {f'dimension_{dim}': None for dim in dims}\n    for dim in dims:\n        mat = extract_submatrix_of_simplices(simplex_list[dim], N, position=position)\n        if simplex_type in ('undirected', 'reciprocal'):\n            mat = (mat + mat.T).astype(bool)\n        skeleton_mats[f'dimension_{dim}'] = mat\n    return skeleton_mats\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.list_simplices_by_dimension","title":"<code>list_simplices_by_dimension(adj, node_properties = None, max_simplices = False, max_dim = -1, nodes = None, verbose = False, simplex_type = 'directed', **kwargs)</code>","text":"<p>List all simplex motifs in the network adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d (N,N)-array or sparse matrix</code> <p>Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code> data frame</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <code>nodes</code> <code>1d array or None(default)</code> <p>Restrict to list only the simplices whose source node is in nodes.  If None list all simplices</p> <code>None</code> <p>Returns:</p> Type Description <code>series</code> <p>Simplex lists indexed per dimension.  The dimension k entry is a (no. of k-simplices, k+1)-array is given, where each row denotes a simplex.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> <code>AssertionError</code> <p>If nodes is not a subarray of np.arange(N)</p>"},{"location":"network/#src.connalysis.network.topology.list_simplices_by_dimension--see-also","title":"See Also","text":"<p>simplex_counts : A function that counts the simplices instead of listing them and has descriptions of the simplex types.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def list_simplices_by_dimension(adj, node_properties=None, max_simplices=False,max_dim=-1,nodes=None,\n                                verbose=False, simplex_type='directed', **kwargs):\n\"\"\"List all simplex motifs in the network adj.\n    Parameters\n    ----------\n    adj : 2d (N,N)-array or sparse matrix\n        Adjacency matrix of a directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties :  data frame\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type : string\n        Type of simplex to consider:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n    nodes : 1d array or None(default)\n        Restrict to list only the simplices whose source node is in nodes.  If None list all simplices\n\n    Returns\n    -------\n    series\n        Simplex lists indexed per dimension.  The dimension k entry is a (no. of k-simplices, k+1)-array\n        is given, where each row denotes a simplex.\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    AssertionError\n        If nodes is not a subarray of np.arange(N)\n\n    See Also\n    --------\n    simplex_counts : A function that counts the simplices instead of listing them and has descriptions of the\n    simplex types.\n    \"\"\"\n    LOG.info(\"COMPUTE list of %ssimplices by dimension\", \"max-\" if max_simplices else \"\")\n\n    import pyflagsercount\n\n    adj=sp.csr_matrix(adj)\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n    if not nodes is None:\n        assert np.isin(nodes,np.arange(N)).all(), \"nodes must be a subarray of the nodes of the matrix\"\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    n_threads = kwargs.get(\"threads\", kwargs.get(\"n_threads\", 1))\n\n\n    # Only the simplices that have sources stored in this temporary file will be considered\n    if not nodes is None:\n        import tempfile\n        import os\n        tmp_file = tempfile.NamedTemporaryFile(delete=False)\n        vertices_todo = tmp_file.name + \".npy\"\n        np.save(vertices_todo, nodes, allow_pickle=False)\n    else:\n        vertices_todo=''\n\n    #Generate simplex_list\n    original=pyflagsercount.flagser_count(adj, max_simplices=max_simplices,threads=n_threads,max_dim=max_dim,\n                                      vertices_todo=vertices_todo, return_simplices=True)['simplices']\n\n    #Remove temporary file\n    if not nodes is None:\n        os.remove(vertices_todo)\n\n    #Format output\n    max_dim = len(original)\n    dims = pd.Index(np.arange(max_dim), name=\"dim\")\n    simplices = pd.Series(original, name=\"simplices\", index=dims).apply(np.array)\n    #When counting all simplices flagser doesn't list dim 0 and 1 because they correspond to vertices and edges\n    if not max_simplices:\n        if nodes is None:\n            nodes=np.arange(0, N)\n        coom = adj.tocoo()\n        simplices[0] = np.reshape(nodes, (nodes.size, 1))\n        mask=np.isin(coom.row,nodes)\n        simplices[1] = np.stack([coom.row[mask], coom.col[mask]]).T\n    return simplices\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.node_degree","title":"<code>node_degree(adj, node_properties = None, direction = None, weighted = False, **kwargs)</code>","text":"<p>Compute degree of nodes in network adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].</p> required <code>node_properties</code> <code>data frame</code> <p>Data frame of neuron properties in adj. Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>direction</code> <code>string or tuple of strings</code> <p>Direction for which to compute the degree</p> <p>'IN' - In degree</p> <p>'OUT'- Out degree</p> <p>None or ('IN', 'OUT') - Total degree i.e. IN+OUT</p> <code>None</code> <p>Returns:</p> Type Description <code>series or data frame</code> <p>Raises:</p> Type Description <code>Warning</code> <p>If adj has non-zero entries in the diagonal</p> <code>AssertionError</code> <p>If direction is invalid</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def node_degree(adj, node_properties=None, direction=None, weighted=False, **kwargs):\n\"\"\"Compute degree of nodes in network adj\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].\n    node_properties : data frame\n        Data frame of neuron properties in adj. Only necessary if used in conjunction with TAP or connectome utilities.\n    direction : string or tuple of strings\n        Direction for which to compute the degree\n\n        'IN' - In degree\n\n        'OUT'- Out degree\n\n        None or ('IN', 'OUT') - Total degree i.e. IN+OUT\n\n    Returns\n    -------\n    series or data frame\n\n    Raises\n    ------\n    Warning\n        If adj has non-zero entries in the diagonal\n    AssertionError\n        If direction is invalid\n    \"\"\"\n    assert not direction or direction in (\"IN\", \"OUT\") or tuple(direction) == (\"IN\", \"OUT\"),\\\n        f\"Invalid `direction`: {direction}\"\n\n    if not isinstance(adj, np. ndarray):\n        matrix = adj.toarray()\n    else:\n        matrix=adj.copy()\n    if not weighted:\n        matrix=matrix.astype('bool')\n    if np.count_nonzero(np.diag(matrix)) != 0:\n        logging.warning('The diagonal is non-zero!  This may cause errors in the analysis')\n    index = pd.Series(range(matrix.shape[0]), name=\"node\")\n    series = lambda array: pd.Series(array, index)\n    in_degree = lambda: series(matrix.sum(axis=0))\n    out_degree = lambda: series(matrix.sum(axis=1))\n\n    if not direction:\n        return in_degree() + out_degree()\n\n    if tuple(direction) == (\"IN\", \"OUT\"):\n        return pd.DataFrame({\"IN\": in_degree(), \"OUT\": out_degree()})\n\n    if tuple(direction) == (\"OUT\", \"IN\"):\n        return pd.DataFrame({\"OUT\": out_degree(), \"IN\": in_degree()})\n\n    return in_degree() if direction == \"IN\" else out_degree()\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.node_k_degree","title":"<code>node_k_degree(adj, node_properties = None, direction = ('IN', 'OUT'), max_dim = -1, **kwargs)</code>","text":"<p>Compute generalized degree of nodes in network adj.  The k-(in/out)-degree of a node v is the number of k-simplices with all its nodes mapping to/from the node v.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>direction</code> <code>string</code> <p>Direction for which to compute the degree</p> <p>'IN' - In degree</p> <p>'OUT'- Out degree</p> <p>(\u2019IN\u2019, \u2019OUT\u2019) - both</p> <code>('IN', 'OUT')</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension for which to compute the degree max_dim &gt;=2 or -1 in which case it computes all dimensions.</p> <code>-1</code> <p>Returns:</p> Type Description <code>data frame</code> <p>Table of of k-(in/out)-degrees</p> <p>Raises:</p> Type Description <code>Warning</code> <p>If adj has non-zero entries in the diagonal which are ignored in the analysis</p> <code>AssertionError</code> <p>If direction is invalid</p> <code>AssertionError</code> <p>If not max_dim &gt;1</p>"},{"location":"network/#src.connalysis.network.topology.node_k_degree--notes","title":"Notes","text":"<p>Note that the k-in-degree of a node v is the number of (k+1) simplices the node v is a sink of. Dually, the k-out-degree of a node v is the number of (k+1) simplices the node v is a source of.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def node_k_degree(adj, node_properties=None, direction=(\"IN\", \"OUT\"), max_dim=-1, **kwargs):\n    #TODO: Generalize from one population to another\n\"\"\"Compute generalized degree of nodes in network adj.  The k-(in/out)-degree of a node v is the number of\n    k-simplices with all its nodes mapping to/from the node v.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    direction : string\n        Direction for which to compute the degree\n\n        'IN' - In degree\n\n        'OUT'- Out degree\n\n        (\u2019IN\u2019, \u2019OUT\u2019) - both\n    max_dim : int\n        Maximal dimension for which to compute the degree max_dim &gt;=2 or -1 in\n        which case it computes all dimensions.\n\n    Returns\n    -------\n    data frame\n        Table of of k-(in/out)-degrees\n\n    Raises\n    ------\n    Warning\n        If adj has non-zero entries in the diagonal which are ignored in the analysis\n    AssertionError\n        If direction is invalid\n    AssertionError\n        If not max_dim &gt;1\n\n    Notes\n    -----\n    Note that the k-in-degree of a node v is the number of (k+1) simplices the node v is a sink of.\n    Dually, the k-out-degree of a node v is the number of (k+1) simplices the node v is a source of.\n    \"\"\"\n    matrix = sp.csr_matrix(adj)\n    assert (max_dim &gt; 1) or (max_dim==-1), \"max_dim should be &gt;=2\"\n    assert direction in (\"IN\", \"OUT\") or tuple(direction) == (\"IN\", \"OUT\"), \\\n        f\"Invalid `direction`: {direction}\"\n    if np.count_nonzero(matrix.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero!  Non-zero entries in the diagonal will be ignored.')\n    import pyflagsercount\n    flagser_out = pyflagsercount.flagser_count(matrix, return_simplices=True, max_dim=max_dim)\n    max_dim_possible = len(flagser_out['cell_counts']) - 1\n    if max_dim==-1:\n        max_dim = max_dim_possible\n    elif max_dim &gt; max_dim_possible:\n        logging.warning(\"The maximum dimension selected is not attained\")\n        max_dim = max_dim_possible\n    if (max_dim &lt;= 1) and (max_dim!=-1):\n        print(\"There are no simplices of dimension 2 or higher\")\n    else:\n        index = pd.Series(range(matrix.shape[0]), name=\"node\")\n        generalized_degree = pd.DataFrame(index=index)\n        for dim in np.arange(2, max_dim + 1):\n            if \"OUT\" in direction:\n                # getting source participation across dimensions\n                x, y = np.unique(np.array(flagser_out['simplices'][dim])[:, 0], return_counts=True)\n                generalized_degree[f'{dim}_out_degree'] = pd.Series(y, index=x)\n            if \"IN\" in direction:\n                # getting sink participation across dimensions\n                x, y = np.unique(np.array(flagser_out['simplices'][dim])[:, dim], return_counts=True)\n                generalized_degree[f'{dim}_in_degree'] = pd.Series(y, index=x)\n        return generalized_degree.fillna(0)\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.node_participation","title":"<code>node_participation(adj, node_properties = None, max_simplices = False, threads = 1, max_dim = -1, simplex_type = 'directed', **kwargs)</code>","text":"<p>Compute the number of simplex motifs in the network adj each node is part of. See simplex_counts for details.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j. The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False (default) counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <code>string</code> <p>Type of simplex to consider:</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <p>Returns:</p> Type Description <code>data frame</code> <p>Indexed by the nodes in adj and with columns de dimension for which node participation is counted</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def node_participation(adj, node_properties=None, max_simplices=False,\n                       threads=1,max_dim=-1,simplex_type='directed',**kwargs):\n\"\"\"Compute the number of simplex motifs in the network adj each node is part of.\n    See simplex_counts for details.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n        The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False (default) counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type : string\n        Type of simplex to consider:\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n\n    Returns\n    -------\n    data frame\n        Indexed by the nodes in adj and with columns de dimension for which node participation is counted\n\n    Raises\n    -------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n    \"\"\"\n\n    adj=sp.csr_matrix(adj).astype('bool')\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    flagser_counts = _flagser_counts(adj, count_node_participation=True, threads=threads,\n                                     max_simplices=max_simplices, max_dim=max_dim)\n    return flagser_counts[\"node_participation\"]\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.normalized_simplex_counts","title":"<code>normalized_simplex_counts(adj, node_properties = None, max_simplices = False, threads = 1, max_dim = -1, **kwargs)</code>","text":"<p>Compute the ratio of directed/undirected simplex counts normalized to be between 0 and 1. See simplex_counts and undirected_simplex_counts for details.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <p>Returns:</p> Type Description <code>panda series</code> <p>Normalized simplex counts</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p>"},{"location":"network/#src.connalysis.network.topology.normalized_simplex_counts--notes","title":"Notes","text":"<p>Maybe we should say why we choose this metric</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def normalized_simplex_counts(adj, node_properties=None,\n                   max_simplices=False, threads=1,max_dim=-1,\n                   **kwargs):\n\"\"\"Compute the ratio of directed/undirected simplex counts normalized to be between 0 and 1.\n    See simplex_counts and undirected_simplex_counts for details.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n\n    Returns\n    -------\n    panda series\n        Normalized simplex counts\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n\n    Notes\n    -----\n    Maybe we should say why we choose this metric\"\"\"\n\n    from scipy.special import factorial\n    denominator=simplex_counts(adj, node_properties=node_properties,max_simplices=max_simplices,\n                                          threads=threads,max_dim=max_dim,simplex_type='undirected', **kwargs).to_numpy()\n    #Global maximum dimension since every directed simplex has an underlying undirected one of the same dimension\n    max_dim_global=denominator.size\n    #Maximum number of possible directed simplices for each undirected simplex across dimensions\n    max_possible_directed=np.array([factorial(i+1) for i in np.arange(max_dim_global)])\n    denominator=np.multiply(denominator, max_possible_directed)\n    numerator=simplex_counts(adj, node_properties=node_properties,max_simplices=max_simplices,\n                             threads=threads,max_dim=max_dim,simple_type='directed', **kwargs).to_numpy()\n    numerator=np.pad(numerator, (0, max_dim_global-len(numerator)), 'constant', constant_values=0)\n    return _series_by_dim(np.divide(numerator,denominator)[1:],name=\"normalized_simplex_counts\",\n                          index=np.arange(1,max_dim_global), name_index=\"dim\")\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.rc_submatrix","title":"<code>rc_submatrix(adj)</code>","text":"<p>Returns the symmetric submatrix of reciprocal connections of adj</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.</p> required <p>Returns:</p> Type Description <code>sparse matrix</code> <p>symmetric matrix of the same dtype as adj of reciprocal connections</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def rc_submatrix(adj):\n\"\"\"Returns the symmetric submatrix of reciprocal connections of adj\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j.\n\n    Returns\n    -------\n    sparse matrix\n        symmetric matrix of the same dtype as adj of reciprocal connections\n    \"\"\"\n    adj=sp.csr_matrix(adj)\n    if np.count_nonzero(adj.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero and this may lead to errors!')\n    mask=adj.copy().astype('bool')\n    mask=(mask.multiply(mask.T))\n    mask.eliminate_zeros\n    return adj.multiply(mask).astype(adj.dtype)\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.simplex_counts","title":"<code>simplex_counts(adj, node_properties = None, max_simplices = False, threads = 1, max_dim = -1, simplex_type = 'directed', **kwargs)</code>","text":"<p>Compute the number of simplex motifs in the network adj.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.</p> required <code>node_properties</code> <code>dataframe</code> <p>Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.</p> <code>None</code> <code>max_simplices</code> <code>bool</code> <p>If False counts all simplices in adj. If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.</p> <code>False</code> <code>max_dim</code> <code>int</code> <p>Maximal dimension up to which simplex motifs are counted. The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.</p> <code>-1</code> <code>simplex_type</code> <p>Type of simplex to consider (See Notes):</p> <p>\u2019directed\u2019 - directed simplices</p> <p>\u2019undirected\u2019 - simplices in the underlying undirected graph</p> <p>\u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections</p> <code>'directed'</code> <p>Returns:</p> Type Description <code>series</code> <p>simplex counts</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If adj has non-zero entries in the diagonal which can produce errors.</p> <code>AssertionError</code> <p>If adj is not square.</p>"},{"location":"network/#src.connalysis.network.topology.simplex_counts--notes","title":"Notes","text":"<p>A directed simplex of dimension k in adj is a set of (k+1) nodes which are all to all connected in a feedforward manner. That is, they can be ordered from 0 to k such that there is an edge from i to j whenever i &lt; j.</p> <p>An undirected simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all connected.  That is, they are all to all connected in the underlying undirected graph of adj.  In the literature this is also called a (k+1)-clique of the underlying undirected graph.</p> <p>A reciprocal simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all reciprocally connected. That is, they are all to all connected in the undirected graph of reciprocal connections of adj.  In the literature this is also called a (k+1)-clique of the undirected graph of reciprocal connections.</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def simplex_counts(adj, node_properties=None,max_simplices=False,\n                   threads=1,max_dim=-1, simplex_type='directed', **kwargs):\n\"\"\"Compute the number of simplex motifs in the network adj.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry adj[i,j] implies there is an edge from i to j\n        of weight adj[i,j].  The matrix can be asymmetric, but must have 0 in the diagonal.\n    node_properties : dataframe\n        Data frame of neuron properties in adj.  Only necessary if used in conjunction with TAP or connectome utilities.\n    max_simplices : bool\n        If False counts all simplices in adj.\n        If True counts only maximal simplices i.e., simplex motifs that are not contained in higher dimensional ones.\n    max_dim : int\n        Maximal dimension up to which simplex motifs are counted.\n        The default max_dim = -1 counts all existing dimensions.  Particularly useful for large or dense graphs.\n    simplex_type: string\n        Type of simplex to consider (See Notes):\n\n        \u2019directed\u2019 - directed simplices\n\n        \u2019undirected\u2019 - simplices in the underlying undirected graph\n\n        \u2019reciprocal\u2019 - simplices in the undirected graph of reciprocal connections\n\n    Returns\n    -------\n    series\n        simplex counts\n\n    Raises\n    ------\n    AssertionError\n        If adj has non-zero entries in the diagonal which can produce errors.\n    AssertionError\n        If adj is not square.\n\n    Notes\n    -----\n    A directed simplex of dimension k in adj is a set of (k+1) nodes which are all to all connected in a feedforward manner.\n    That is, they can be ordered from 0 to k such that there is an edge from i to j whenever i &lt; j.\n\n    An undirected simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all connected.  That is, they\n    are all to all connected in the underlying undirected graph of adj.  In the literature this is also called a (k+1)-clique\n    of the underlying undirected graph.\n\n    A reciprocal simplex of dimension k in adj is a set of (k+1) nodes in adj which are all to all reciprocally connected.\n    That is, they are all to all connected in the undirected graph of reciprocal connections of adj.  In the literature this is\n    also called a (k+1)-clique of the undirected graph of reciprocal connections.\n    \"\"\"\n    adj=sp.csr_matrix(adj)\n    assert np.count_nonzero(adj.diagonal()) == 0, 'The diagonal of the matrix is non-zero and this may lead to errors!'\n    N, M = adj.shape\n    assert N == M, 'Dimension mismatch. The matrix must be square.'\n\n\n    #Symmetrize matrix if simplex_type is not 'directed'\n    if simplex_type=='undirected':\n        adj=sp.triu(underlying_undirected_matrix(adj)) #symmtrize and keep upper triangular only\n    elif simplex_type==\"reciprocal\":\n        adj=sp.triu(rc_submatrix(adj)) #symmtrize and keep upper triangular only\n\n    flagser_counts = _flagser_counts(adj, threads=threads, max_simplices=max_simplices, max_dim=max_dim)\n    if max_simplices:\n        return flagser_counts[\"max_simplex_counts\"]\n    else:\n        return flagser_counts[\"simplex_counts\"]\n</code></pre>"},{"location":"network/#src.connalysis.network.topology.underlying_undirected_matrix","title":"<code>underlying_undirected_matrix(adj)</code>","text":"<p>Returns the symmetric matrix of undirected connections of <code>adj</code>.</p> <p>Parameters:</p> Name Type Description Default <code>adj</code> <code>2d array or sparse matrix</code> <p>Adjacency matrix of the directed network.  A non-zero entry in <code>adj[i][j]</code> implies there is an edge from vertex <code>i</code> to vertex <code>j</code>.</p> required <p>Returns:</p> Type Description <code>sparse boolean matrix</code> <p>Corresponding to the symmetric underlying undirected graph</p> Source code in <code>src/connalysis/network/topology.py</code> <pre><code>def underlying_undirected_matrix(adj):\n\"\"\"Returns the symmetric matrix of undirected connections of `adj`.\n    Parameters\n    ----------\n    adj : 2d array or sparse matrix\n        Adjacency matrix of the directed network.  A non-zero entry in `adj[i][j]` implies there is an edge from vertex `i` to vertex `j`.\n\n    Returns\n    -------\n    sparse boolean matrix\n        Corresponding to the symmetric underlying undirected graph\n    \"\"\"\n    adj=sp.csr_matrix(adj)\n    if np.count_nonzero(adj.diagonal()) != 0:\n        logging.warning('The diagonal is non-zero and this may lead to errors!')\n    return (adj+adj.T).astype('bool')\n</code></pre>"},{"location":"randomization/","title":"Functions for generating random models","text":"<p>This page describes functions contained in the <code>randomization</code> module.</p>"},{"location":"randomization/#src.connalysis.randomization.randomization.ER_shuffle","title":"<code>ER_shuffle(adj, neuron_properties = [])</code>","text":""},{"location":"randomization/#src.connalysis.randomization.randomization.ER_shuffle--creates-an-er-control-by-shuffling-entries-away-from-the-diagonal-in-adj","title":"Creates an ER control by shuffling entries away from the diagonal in adj","text":"<p>TODO: Re-implement this using only sparse matrices</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>@seed_random_state\ndef ER_shuffle(adj, neuron_properties=[]):\n\"\"\"\n    #Creates an ER control by shuffling entries away from the diagonal in adj\n    TODO: Re-implement this using only sparse matrices\n    \"\"\"\n    n = adj.get_shape()[0]\n    adj = adj.toarray()\n    LOG.info(\"Shuffle %s edges following Erdos-Renyi\", adj.sum())\n    above_diagonal = adj[np.triu_indices(n, k=1)]\n    below_diagonal = adj[np.tril_indices(n, k=-1)]\n    off_diagonal = np.concatenate([above_diagonal, below_diagonal])\n\n    np.random.shuffle(off_diagonal)\n    adj[np.triu_indices(n,k=1)] = off_diagonal[0:n*(n-1)//2]\n    adj[np.tril_indices(n,k=-1)] = off_diagonal[n*(n-1)//2:]\n    return sp.csr_matrix(adj)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.adjusted_ER","title":"<code>adjusted_ER(sparse_matrix: sp.csc_matrix, generator_seed: int) -&gt; sp.csc_matrix</code>","text":"<p>Function to generate an ER with adjusted bidirectional connections.</p> <p>:param sparse_matrix: Sparse input matrix. :type: sp.csc_matrix :param generator_seed: Numpy generator seed. :type: int</p> <p>:return adjER_matrix: Adjusted ER model. :rtype: sp.csc_matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def adjusted_ER(sparse_matrix: sp.csc_matrix, generator_seed:int) -&gt; sp.csc_matrix:\n\"\"\"\n    Function to generate an ER with adjusted bidirectional connections.\n\n    :param sparse_matrix: Sparse input matrix.\n    :type: sp.csc_matrix\n    :param generator_seed: Numpy generator seed.\n    :type: int\n\n    :return adjER_matrix: Adjusted ER model.\n    :rtype: sp.csc_matrix\n    \"\"\"\n    from .resources.randomization import bidrectional_edges, adjust_bidirectional_connections\n    generator = np.random.default_rng(generator_seed)\n    target_bedges = int(bidirectional_edges(sparse_matrix).count_nonzero() / 2)\n    ER_matrix = ER_shuffle(sparse_matrix).tocsc()\n    return adjust_bidirectional_connections(ER_matrix, target_bedges, generator)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.bishuffled_model","title":"<code>bishuffled_model(sparse_matrix: sp.csc_matrix, generator_seed: int) -&gt; sp.csc_matrix</code>","text":"<p>Function to generate the bishuffled control model, obtained by removing bidirectional edges by assigning a direction (according to GID order) to exactly half of them and the other direction to the other half. Probability-based direction assignment (original thesis) was not implemented to make the algorithm more performant, maybe to be used for SSCX.</p> <p>:param sparse_matrix: Sparse input matrix. :type: sp.csc_matrix :param generator_seed: Numpy generator seed. :type: int</p> <p>:return und_matrix: Bishuffled model. :rtype: sp.csc_matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def bishuffled_model(sparse_matrix: sp.csc_matrix, generator_seed: int) -&gt; sp.csc_matrix:\n\"\"\"\n    Function to generate the bishuffled control model, obtained by removing\n    bidirectional edges by assigning a direction (according to GID order)\n    to exactly half of them and the other direction to the other half.\n    Probability-based direction assignment (original thesis) was not implemented\n    to make the algorithm more performant, maybe to be used for SSCX.\n\n    :param sparse_matrix: Sparse input matrix.\n    :type: sp.csc_matrix\n    :param generator_seed: Numpy generator seed.\n    :type: int\n\n    :return und_matrix: Bishuffled model.\n    :rtype: sp.csc_matrix\n    \"\"\"\n    from .resources.randomization import bidrectional_edges, add_bidirectional_connections, half_matrix\n    generator = np.random.default_rng(generator_seed)\n    ut_bedges = sp.triu(bidirectional_edges(sparse_matrix))\n    target_bedges = ut_bedges.count_nonzero()\n    bedges1, bedges2 = half_matrix(ut_bedges, generator)\n    return add_bidirectional_connections(sparse_matrix - bedges1 - bedges2.T, target_bedges, generator)\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.configuration_model","title":"<code>configuration_model(sparse_matrix: sp.coo_matrix, generator_seed: int)</code>","text":"<p>Function to generate the configuration control model, obtained by shuffling the row and column of coo format independently, to create new coo matrix, then removing any multiple edges and loops.</p> <p>:param sparse_matrix: Sparse input matrix. :type: sp.coo_matrix :param generator_seed: Numpy generator seed. :type: int</p> <p>:return CM_matrix: Configuration model. :rtype: sp.csr_matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def configuration_model(sparse_matrix: sp.coo_matrix, generator_seed: int):\n\"\"\"\n    Function to generate the configuration control model, obtained by\n    shuffling the row and column of coo format independently, to create\n    new coo matrix, then removing any multiple edges and loops.\n\n    :param sparse_matrix: Sparse input matrix.\n    :type: sp.coo_matrix\n    :param generator_seed: Numpy generator seed.\n    :type: int\n\n    :return CM_matrix: Configuration model.\n    :rtype: sp.csr_matrix\n    \"\"\"\n    generator = np.random.default_rng(generator_seed)\n    R = sparse_matrix.row\n    C = sparse_matrix.col\n    generator.shuffle(R)\n    generator.shuffle(C)\n    CM_matrix = sp.coo_matrix(([1]*len(R),(R,C)),shape=sparse_matrix.shape).tocsr()\n    CM_matrix.setdiag(0)\n    CM_matrix.eliminate_zeros()\n    return CM_matrix\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2","title":"<code>run_DD2(n, a, b, xyz, threads, seed = (None, None))</code>","text":"<p>Distance Dependant 2nd Order</p> <p>Input: n, a, b, xyz, threads    n = number of vertices (int)    a = coefficient of probability function (double)    b = coefficient of probability function (double)    xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)    threads = number of threads to use (int)</p> <p>Output: coo matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2(n,a,b,xyz,threads, seed=(None,None)):\n\"\"\"\n    Distance Dependant 2nd Order\n\n    Input: n, a, b, xyz, threads\n       n = number of vertices (int)\n       a = coefficient of probability function (double)\n       b = coefficient of probability function (double)\n       xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)\n       threads = number of threads to use (int)\n\n    Output: coo matrix\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        return gm.DD2(n,a,b,xyz,threads)\n    else:\n        return gm.DD2(n,a,b,xyz,threads,seed[0],seed[1])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2_block","title":"<code>run_DD2_block(n, pathways, mtypes, xyz, threads, seed = (None, None))</code>","text":"<p>Distance Dependant Stochastic Block Model</p> <p>Input: n, pathways, mtypes, xyz, threads    n = number of vertices (int)    pathways = pathwaysi is a pair (a,b) of coefficients for DD2 probability function (numpy array, shape=(m,m,2), dtype=double), where m is number of mtypes    mtypes = i'th entry is mtype of vertex i (numpy array, shape=(n,), dtype=uint8)    xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)    threads = number of threads to use (int)</p> <p>Output: coo matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2_block(n,pathways,mtypes,xyz,threads, seed=(None,None)):\n\"\"\"\n    Distance Dependant Stochastic Block Model\n\n    Input: n, pathways, mtypes, xyz, threads\n       n = number of vertices (int)\n       pathways = pathways[i][j] is a pair (a,b) of coefficients for DD2 probability function (numpy array, shape=(m,m,2), dtype=double), where m is number of mtypes\n       mtypes = i'th entry is mtype of vertex i (numpy array, shape=(n,), dtype=uint8)\n       xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)\n       threads = number of threads to use (int)\n\n    Output: coo matrix\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        return gm.DD2_block(n,pathways,mtypes,xyz,threads)\n    else:\n        return gm.DD2_block(n,pathways,mtypes,xyz,threads,seed[0],seed[1])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2_block_pre","title":"<code>run_DD2_block_pre(n, pathways, mtypes, xyz, threads, seed = (None, None))</code>","text":"<p>Distance Dependant Stochastic Block Model (pre synaptic only)</p> <p>Input: n, pathways, mtypes, xyz, threads    n = number of vertices (int)    pathways = pathways[i] is a pair (a,b) of coefficients for DD2 probability function (numpy array, shape=(m,2), dtype=double), where m is number of mtypes    mtypes = i'th entry is mtype of vertex i (numpy array, shape=(n,), dtype=uint8)    xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)    threads = number of threads to use (int)</p> <p>Output: coo matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2_block_pre(n,pathways,mtypes,xyz,threads, seed=(None,None)):\n\"\"\"\n    Distance Dependant Stochastic Block Model (pre synaptic only)\n\n    Input: n, pathways, mtypes, xyz, threads\n       n = number of vertices (int)\n       pathways = pathways[i] is a pair (a,b) of coefficients for DD2 probability function (numpy array, shape=(m,2), dtype=double), where m is number of mtypes\n       mtypes = i'th entry is mtype of vertex i (numpy array, shape=(n,), dtype=uint8)\n       xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)\n       threads = number of threads to use (int)\n\n    Output: coo matrix\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        return gm.DD2_block_pre(n,pathways,mtypes,xyz,threads)\n    else:\n        gm.DD2_block_pre(n,pathways,mtypes,xyz,threads,seed[0],seed[1])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD2_model","title":"<code>run_DD2_model(adj, node_properties, model_params_dd2 = None, coord_names = ['x', 'y', 'z'], threads = 8, return_params = False, **config_dict)</code>","text":"<p>Wrapper generating a random control graph based on 2nd order distance dependence model Input: adj: original adjacency matrix, if model_params have already been computed can pass empty matrix of the right size node_properties: DataFrame with information on the vertices of adj, it must have columns corresponding to the names the coordinates to be used for distance computation.  Default ['x', 'y', 'z'] configdict: Add me --&gt; to generate parameters of 2nd order distance model model_params: optional input of pre-computed model parameters, data frame with rows corresponding to seeds of model estimation (single row if subsampling is not used) and columns: exp_model_scale and exp_model_exponent for the model parameters.  See modelling.conn_prob_2nd_order_model for details.</p> <p>Output: scipy coo matrix, optional model_parameters</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD2_model(adj, node_properties,\n                  model_params_dd2=None, #an analysis that could be loaded from the pipeline\n                  coord_names= ['x', 'y', 'z'],\n                  threads=8, return_params=False, **config_dict):\n\"\"\"\n    Wrapper generating a random control graph based on 2nd order distance dependence model\n    Input:\n    adj: original adjacency matrix, if model_params have already been computed can pass empty matrix of the right size\n    node_properties: DataFrame with information on the vertices of adj, it must have columns corresponding to the names\n    the coordinates to be used for distance computation.  Default ['x', 'y', 'z']\n    configdict: Add me --&gt; to generate parameters of 2nd order distance model\n    model_params: optional input of pre-computed model parameters, data frame with rows corresponding to seeds of model estimation\n    (single row if subsampling is not used) and columns:\n    exp_model_scale and exp_model_exponent for the model parameters.  See modelling.conn_prob_2nd_order_model for details.\n\n    Output: scipy coo matrix, optional model_parameters\n    \"\"\"\n\n    if model_params_dd2 is None:\n        from .import modelling\n        #TODO:  What to do if coord_names are also given in configdict and do not match coord_names?\n        config_dict[\"coord_names\"]=coord_names\n        model_params_dd2 = modelling.conn_prob_2nd_order_model(adj, node_properties,**config_dict)\n\n    LOG.info(\"Run DD2 model with parameters: \\n%s\", model_params_dd2)\n\n    n = adj.shape[0]\n    a = model_params_dd2.mean(axis=0)['exp_model_scale']\n    b = model_params_dd2.mean(axis=0)['exp_model_exponent']\n    xyz = node_properties.loc[:,coord_names].to_numpy() #Make and assert that checks these columns exist!\n    if len(coord_names)&lt;3: #Extend by zeros if lower dimensional data was used to compute distance\n        xyz=np.hstack([xyz,np.zeros((xyz.shape[0],3-xyz.shape[1]))])\n    C=gm.DD2(n,a,b,xyz,threads)\n    i=C['row']\n    j=C['col']\n    data=np.ones(len(i))\n    if return_params==True:\n        return sp.coo_matrix((data, (i, j)), [n,n]), model_params_dd2\n    else:\n        return sp.coo_matrix((data, (i, j)), [n,n])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_DD3","title":"<code>run_DD3(n, a1, b1, a2, b2, xyz, depths, threads, seed = (None, None))</code>","text":"<p>Distance Dependant 3rd Order</p> <p>Input: n, a, b, xyz, threads    n = number of vertices (int)    a1 and a2 = coefficients of probability function for dz&lt;0 (double)    a2 and b2 = coefficient of probability function for dz&gt;0 (double)    xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)    depths = i'th entry is depth of vertex i (numpy array, shape=(n,), dtype=float64)    threads = number of threads to use (int)</p> <p>Output: coo matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_DD3(n,a1,b1,a2,b2,xyz,depths,threads, seed=(None,None)):\n\"\"\"\n    Distance Dependant 3rd Order\n\n    Input: n, a, b, xyz, threads\n       n = number of vertices (int)\n       a1 and a2 = coefficients of probability function for dz&lt;0 (double)\n       a2 and b2 = coefficient of probability function for dz&gt;0 (double)\n       xyz = the coordinates of the vertices, (numpy array, shape=(n,3), dtype=float64)\n       depths = i'th entry is depth of vertex i (numpy array, shape=(n,), dtype=float64)\n       threads = number of threads to use (int)\n\n    Output: coo matrix\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        return gm.DD3(n,a1,b1,a2,b2,xyz,depths,threads)\n    else:\n        return gm.DD3(n,a1,b1,a2,b2,xyz,depths,threads,seed[0],seed[1])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_ER","title":"<code>run_ER(n, p, threads = 8, seed = (None, None))</code>","text":"<p>Creates an Erdos Renyi digraph.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>p</code> <code>float</code> <p>Edge probablity, must satisfy \\(0 \\le p \\le 1\\)</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>dict</code> <p>The edge list of the new digraph as a dictionary with keys 'row' and 'col'. Where (row[i],col[i]) is a directed edge of the digraph, for all i.</p> <p>Examples:</p> <p>Setting n=3 and p=1 gives the complete digraph on 3 vertices:</p> <pre><code>&gt;&gt;&gt; connalysis.randomization.run_ER(3,1)\n{'row': [0, 0, 1, 1, 2, 2], 'col': [1, 2, 0, 2, 0, 1]}\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If p is not between 0 and 1</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_ER(n, p, threads=8, seed=(None,None)):\n\"\"\"Creates an Erdos Renyi digraph.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    p : float\n        Edge probablity, must satisfy $0 \\\\le p \\\\le 1$\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    dict\n        The edge list of the new digraph as a dictionary\n        with keys 'row' and 'col'. Where (row[i],col[i]) is a directed edge\n        of the digraph, for all i.\n\n    Examples\n    --------\n    Setting n=3 and p=1 gives the complete digraph on 3 vertices:\n    &gt;&gt;&gt; connalysis.randomization.run_ER(3,1)\n    {'row': [0, 0, 1, 1, 2, 2], 'col': [1, 2, 0, 2, 0, 1]}\n\n    Raises\n    ------\n    AssertionError\n        If p is not between 0 and 1\n\n    \"\"\"\n    assert (p &gt;= 0 and p &lt;= 1), \"p must be between 0 and 1\"\n    if seed[0]==None or seed[1]==None:\n        return gm.ER(n,p,threads)\n    else:\n        return gm.ER(n,p,threads,seed[0],seed[1])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.run_SBM","title":"<code>run_SBM(n, pathways, blocks, threads = 8, seed = (None, None))</code>","text":"<p>Creates an random digraph using the Stochastic Block Model.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>Number of vertices</p> required <code>pathways</code> <code>numpy array of floats</code> <p>shape=(m,m) where m is the number of blocks. pathwaysi is probability of an edge between block i and block j</p> required <code>blocks</code> <code>numpy array of ints</code> <p>shape=(n,). The i'th entry gives to which block vertex i belongs.</p> required <code>threads</code> <code>int</code> <p>Number of parallel threads to be used</p> <code>8</code> <code>seed</code> <code>pair of ints</code> <p>Random seed to be used, if none is provided a seed is randomly selected</p> <code>(None, None)</code> <p>Returns:</p> Type Description <code>dict</code> <p>The edge list of the new digraph as a dictionary with keys 'row' and 'col'. Where (row[i],col[i]) is a directed edge of the digraph, for all i.</p> <p>Examples:</p> <p>Here we create an SBM digraph on 4 vertices where the even and odd vertices connect with high probablity (p=0.9) and the even to evens or odd to odds connect with low probability (p=0.1):</p> <pre><code>&gt;&gt;&gt; connalysis.randomization.run_SBM(4,np.array([[0.1,0.9],[0.9,0.1]]),np.array([0,1,0,1]))\n{'row': [0, 0, 1, 1, 1, 2, 2, 3, 3], 'col': [1, 3, 0, 2, 3, 1, 3, 0, 2]\n</code></pre> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If p is not between 0 and 1</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def run_SBM(n, pathways, blocks, threads=8, seed=(None,None)):\n\"\"\"Creates an random digraph using the Stochastic Block Model.\n\n    Parameters\n    ----------\n    n : int\n        Number of vertices\n    pathways : numpy array of floats\n        shape=(m,m) where m is the number of blocks.\n        pathways[i][j] is probability of an edge between block i and block j\n    blocks : numpy array of ints\n        shape=(n,). The i'th entry gives to which block vertex i belongs.\n    threads : int\n        Number of parallel threads to be used\n    seed : pair of ints\n        Random seed to be used, if none is provided a seed is randomly selected\n\n    Returns\n    -------\n    dict\n        The edge list of the new digraph as a dictionary\n        with keys 'row' and 'col'. Where (row[i],col[i]) is a directed edge\n        of the digraph, for all i.\n\n    Examples\n    --------\n    Here we create an SBM digraph on 4 vertices where the even and\n    odd vertices connect with high probablity (p=0.9)\n    and the even to evens or odd to odds connect with low probability (p=0.1):\n    &gt;&gt;&gt; connalysis.randomization.run_SBM(4,np.array([[0.1,0.9],[0.9,0.1]]),np.array([0,1,0,1]))\n    {'row': [0, 0, 1, 1, 1, 2, 2, 3, 3], 'col': [1, 3, 0, 2, 3, 1, 3, 0, 2]\n\n\n    Raises\n    ------\n    AssertionError\n        If p is not between 0 and 1\n\n    \"\"\"\n\"\"\"\n    Stochastic Block Model\n\n    Input: n, pathways, mtypes, threads\n       n = number of vertices (int)\n       pathways = pathways[i][j] entry is probability of edge between mtype i and mtype j (numpy array, shape=(m,m), dtype=float64), where m is number of mtypes\n       mtypes = i'th entry is mtype of vertex i (numpy array, shape=(n,), dtype=uint8)\n       threads = number of threads to use (int)\n\n    Output: coo matrix\n    \"\"\"\n    if seed[0]==None or seed[1]==None:\n        return gm.SBM(n, pathways, blocks, threads)\n    else:\n        return gm.SBM(n, pathways, blocks, threads, seed[0], seed[1])\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.seed_random_state","title":"<code>seed_random_state(shuffler, seeder = np.random.seed)</code>","text":"<p>Decorate a connectivity shuffler to seed it's random-state before execution.</p> <p>It is expected that the generator can be seeded calling <code>seeder(seed)</code>.</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def seed_random_state(shuffler, seeder=np.random.seed):\n\"\"\"Decorate a connectivity shuffler to seed it's random-state before execution.\n\n    It is expected that the generator can be seeded calling `seeder(seed)`.\n    \"\"\"\n    def seed_and_run_method(adj, neuron_properties=[], seed=None, **kwargs):\n\"\"\"Reinitialize numpy random state using the value of seed among `kwargs`.\n        doing nothing if no `seed` provided --- expecting an external initialization.\n        \"\"\"\n        if seed is None:\n            LOG.warning(\"No seed among keyword arguments\")\n        else:\n            seeder(seed)\n\n        return shuffler(adj, neuron_properties, **kwargs)\n\n    return seed_and_run_method\n</code></pre>"},{"location":"randomization/#src.connalysis.randomization.randomization.underlying_model","title":"<code>underlying_model(sparse_matrix: sp.csc_matrix, generator_seed: int)</code>","text":"<p>Function to generate the underlying control model, obtained by turning the graph into a DAG (by making it undirected and using the GIDs to give directions) and adding bidirectional connections.</p> <p>:param sparse_matrix: Sparse input matrix. :type: sp.csc_matrix :param generator_seed: Numpy generator seed. :type: int</p> <p>:return und_matrix: Underlying model. :rtype: sp.csc_matrix</p> Source code in <code>src/connalysis/randomization/randomization.py</code> <pre><code>def underlying_model(sparse_matrix: sp.csc_matrix, generator_seed: int):\n\"\"\"\n    Function to generate the underlying control model, obtained by turning\n    the graph into a DAG (by making it undirected and using the GIDs to give\n    directions) and adding bidirectional connections.\n\n    :param sparse_matrix: Sparse input matrix.\n    :type: sp.csc_matrix\n    :param generator_seed: Numpy generator seed.\n    :type: int\n\n    :return und_matrix: Underlying model.\n    :rtype: sp.csc_matrix\n    \"\"\"\n    from .resources.randomization import bidrectional_edges, add_bidirectional_connections\n    generator = np.random.default_rng(generator_seed)\n    target_bedges = int(bidirectional_edges(sparse_matrix).count_nonzero() / 2)\n    ut_matrix = sp.triu(sparse_matrix + sparse_matrix.T)\n    return add_bidirectional_connections(ut_matrix, target_bedges, generator)\n</code></pre>"}]}